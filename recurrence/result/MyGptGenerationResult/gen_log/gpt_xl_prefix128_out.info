Namespace(batch_size=1, cpu_count=1, data_dir='./data', decode_strategy='random', learning_rate=0.0001, maxlen=35, model_config='./config.json', name='run', num_epochs=20, prefix=0, pretrain_dir=None, temperature=1, test='gpt-xl-prefix128', tokenizer_dir='./tokenizer', top_k=40, top_p=1.0, train_dir='./train_test')
Tokenizer PAD ID: 50256
Loading Data ...
Loading model from ./train_test/checkpoint_gpt-xl-prefix128.pth.tar
TfmrLMHeadModel(
    transformer: TfmrModel(
        wte: Embedding(50257, 1600)
        wpe: Embedding(1024, 1600)
        drop: Dropout(0.1, is_train=False)
        h: Sequential(
            0: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            1: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            2: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            3: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            4: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            5: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            6: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            7: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            8: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            9: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            10: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            11: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            12: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            13: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            14: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            15: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            16: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            17: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            18: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            19: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            20: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            21: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            22: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            23: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            24: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            25: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            26: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            27: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            28: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            29: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            30: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            31: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            32: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            33: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            34: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            35: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            36: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            37: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            38: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            39: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            40: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            41: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            42: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            43: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            44: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            45: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            46: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            47: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
        )
        ln_f: LayerNorm((1600,), 1e-05, elementwise_affine=True)
    )
    lm_head: Linear(1600, 50257, None, None)
)
transformer.wte.weight
transformer.wpe.weight
transformer.h.0.prefix_word
transformer.h.0.ln_1.weight
transformer.h.0.ln_1.bias
transformer.h.0.attn.bias
transformer.h.0.attn.masked_bias
transformer.h.0.attn.c_attn.weight
transformer.h.0.attn.c_attn.bias
transformer.h.0.attn.c_proj.weight
transformer.h.0.attn.c_proj.bias
transformer.h.0.ln_2.weight
transformer.h.0.ln_2.bias
transformer.h.0.mlp.c_fc.weight
transformer.h.0.mlp.c_fc.bias
transformer.h.0.mlp.c_proj.weight
transformer.h.0.mlp.c_proj.bias
transformer.h.1.prefix_word
transformer.h.1.ln_1.weight
transformer.h.1.ln_1.bias
transformer.h.1.attn.bias
transformer.h.1.attn.masked_bias
transformer.h.1.attn.c_attn.weight
transformer.h.1.attn.c_attn.bias
transformer.h.1.attn.c_proj.weight
transformer.h.1.attn.c_proj.bias
transformer.h.1.ln_2.weight
transformer.h.1.ln_2.bias
transformer.h.1.mlp.c_fc.weight
transformer.h.1.mlp.c_fc.bias
transformer.h.1.mlp.c_proj.weight
transformer.h.1.mlp.c_proj.bias
transformer.h.2.prefix_word
transformer.h.2.ln_1.weight
transformer.h.2.ln_1.bias
transformer.h.2.attn.bias
transformer.h.2.attn.masked_bias
transformer.h.2.attn.c_attn.weight
transformer.h.2.attn.c_attn.bias
transformer.h.2.attn.c_proj.weight
transformer.h.2.attn.c_proj.bias
transformer.h.2.ln_2.weight
transformer.h.2.ln_2.bias
transformer.h.2.mlp.c_fc.weight
transformer.h.2.mlp.c_fc.bias
transformer.h.2.mlp.c_proj.weight
transformer.h.2.mlp.c_proj.bias
transformer.h.3.prefix_word
transformer.h.3.ln_1.weight
transformer.h.3.ln_1.bias
transformer.h.3.attn.bias
transformer.h.3.attn.masked_bias
transformer.h.3.attn.c_attn.weight
transformer.h.3.attn.c_attn.bias
transformer.h.3.attn.c_proj.weight
transformer.h.3.attn.c_proj.bias
transformer.h.3.ln_2.weight
transformer.h.3.ln_2.bias
transformer.h.3.mlp.c_fc.weight
transformer.h.3.mlp.c_fc.bias
transformer.h.3.mlp.c_proj.weight
transformer.h.3.mlp.c_proj.bias
transformer.h.4.prefix_word
transformer.h.4.ln_1.weight
transformer.h.4.ln_1.bias
transformer.h.4.attn.bias
transformer.h.4.attn.masked_bias
transformer.h.4.attn.c_attn.weight
transformer.h.4.attn.c_attn.bias
transformer.h.4.attn.c_proj.weight
transformer.h.4.attn.c_proj.bias
transformer.h.4.ln_2.weight
transformer.h.4.ln_2.bias
transformer.h.4.mlp.c_fc.weight
transformer.h.4.mlp.c_fc.bias
transformer.h.4.mlp.c_proj.weight
transformer.h.4.mlp.c_proj.bias
transformer.h.5.prefix_word
transformer.h.5.ln_1.weight
transformer.h.5.ln_1.bias
transformer.h.5.attn.bias
transformer.h.5.attn.masked_bias
transformer.h.5.attn.c_attn.weight
transformer.h.5.attn.c_attn.bias
transformer.h.5.attn.c_proj.weight
transformer.h.5.attn.c_proj.bias
transformer.h.5.ln_2.weight
transformer.h.5.ln_2.bias
transformer.h.5.mlp.c_fc.weight
transformer.h.5.mlp.c_fc.bias
transformer.h.5.mlp.c_proj.weight
transformer.h.5.mlp.c_proj.bias
transformer.h.6.prefix_word
transformer.h.6.ln_1.weight
transformer.h.6.ln_1.bias
transformer.h.6.attn.bias
transformer.h.6.attn.masked_bias
transformer.h.6.attn.c_attn.weight
transformer.h.6.attn.c_attn.bias
transformer.h.6.attn.c_proj.weight
transformer.h.6.attn.c_proj.bias
transformer.h.6.ln_2.weight
transformer.h.6.ln_2.bias
transformer.h.6.mlp.c_fc.weight
transformer.h.6.mlp.c_fc.bias
transformer.h.6.mlp.c_proj.weight
transformer.h.6.mlp.c_proj.bias
transformer.h.7.prefix_word
transformer.h.7.ln_1.weight
transformer.h.7.ln_1.bias
transformer.h.7.attn.bias
transformer.h.7.attn.masked_bias
transformer.h.7.attn.c_attn.weight
transformer.h.7.attn.c_attn.bias
transformer.h.7.attn.c_proj.weight
transformer.h.7.attn.c_proj.bias
transformer.h.7.ln_2.weight
transformer.h.7.ln_2.bias
transformer.h.7.mlp.c_fc.weight
transformer.h.7.mlp.c_fc.bias
transformer.h.7.mlp.c_proj.weight
transformer.h.7.mlp.c_proj.bias
transformer.h.8.prefix_word
transformer.h.8.ln_1.weight
transformer.h.8.ln_1.bias
transformer.h.8.attn.bias
transformer.h.8.attn.masked_bias
transformer.h.8.attn.c_attn.weight
transformer.h.8.attn.c_attn.bias
transformer.h.8.attn.c_proj.weight
transformer.h.8.attn.c_proj.bias
transformer.h.8.ln_2.weight
transformer.h.8.ln_2.bias
transformer.h.8.mlp.c_fc.weight
transformer.h.8.mlp.c_fc.bias
transformer.h.8.mlp.c_proj.weight
transformer.h.8.mlp.c_proj.bias
transformer.h.9.prefix_word
transformer.h.9.ln_1.weight
transformer.h.9.ln_1.bias
transformer.h.9.attn.bias
transformer.h.9.attn.masked_bias
transformer.h.9.attn.c_attn.weight
transformer.h.9.attn.c_attn.bias
transformer.h.9.attn.c_proj.weight
transformer.h.9.attn.c_proj.bias
transformer.h.9.ln_2.weight
transformer.h.9.ln_2.bias
transformer.h.9.mlp.c_fc.weight
transformer.h.9.mlp.c_fc.bias
transformer.h.9.mlp.c_proj.weight
transformer.h.9.mlp.c_proj.bias
transformer.h.10.prefix_word
transformer.h.10.ln_1.weight
transformer.h.10.ln_1.bias
transformer.h.10.attn.bias
transformer.h.10.attn.masked_bias
transformer.h.10.attn.c_attn.weight
transformer.h.10.attn.c_attn.bias
transformer.h.10.attn.c_proj.weight
transformer.h.10.attn.c_proj.bias
transformer.h.10.ln_2.weight
transformer.h.10.ln_2.bias
transformer.h.10.mlp.c_fc.weight
transformer.h.10.mlp.c_fc.bias
transformer.h.10.mlp.c_proj.weight
transformer.h.10.mlp.c_proj.bias
transformer.h.11.prefix_word
transformer.h.11.ln_1.weight
transformer.h.11.ln_1.bias
transformer.h.11.attn.bias
transformer.h.11.attn.masked_bias
transformer.h.11.attn.c_attn.weight
transformer.h.11.attn.c_attn.bias
transformer.h.11.attn.c_proj.weight
transformer.h.11.attn.c_proj.bias
transformer.h.11.ln_2.weight
transformer.h.11.ln_2.bias
transformer.h.11.mlp.c_fc.weight
transformer.h.11.mlp.c_fc.bias
transformer.h.11.mlp.c_proj.weight
transformer.h.11.mlp.c_proj.bias
transformer.h.12.prefix_word
transformer.h.12.ln_1.weight
transformer.h.12.ln_1.bias
transformer.h.12.attn.bias
transformer.h.12.attn.masked_bias
transformer.h.12.attn.c_attn.weight
transformer.h.12.attn.c_attn.bias
transformer.h.12.attn.c_proj.weight
transformer.h.12.attn.c_proj.bias
transformer.h.12.ln_2.weight
transformer.h.12.ln_2.bias
transformer.h.12.mlp.c_fc.weight
transformer.h.12.mlp.c_fc.bias
transformer.h.12.mlp.c_proj.weight
transformer.h.12.mlp.c_proj.bias
transformer.h.13.prefix_word
transformer.h.13.ln_1.weight
transformer.h.13.ln_1.bias
transformer.h.13.attn.bias
transformer.h.13.attn.masked_bias
transformer.h.13.attn.c_attn.weight
transformer.h.13.attn.c_attn.bias
transformer.h.13.attn.c_proj.weight
transformer.h.13.attn.c_proj.bias
transformer.h.13.ln_2.weight
transformer.h.13.ln_2.bias
transformer.h.13.mlp.c_fc.weight
transformer.h.13.mlp.c_fc.bias
transformer.h.13.mlp.c_proj.weight
transformer.h.13.mlp.c_proj.bias
transformer.h.14.prefix_word
transformer.h.14.ln_1.weight
transformer.h.14.ln_1.bias
transformer.h.14.attn.bias
transformer.h.14.attn.masked_bias
transformer.h.14.attn.c_attn.weight
transformer.h.14.attn.c_attn.bias
transformer.h.14.attn.c_proj.weight
transformer.h.14.attn.c_proj.bias
transformer.h.14.ln_2.weight
transformer.h.14.ln_2.bias
transformer.h.14.mlp.c_fc.weight
transformer.h.14.mlp.c_fc.bias
transformer.h.14.mlp.c_proj.weight
transformer.h.14.mlp.c_proj.bias
transformer.h.15.prefix_word
transformer.h.15.ln_1.weight
transformer.h.15.ln_1.bias
transformer.h.15.attn.bias
transformer.h.15.attn.masked_bias
transformer.h.15.attn.c_attn.weight
transformer.h.15.attn.c_attn.bias
transformer.h.15.attn.c_proj.weight
transformer.h.15.attn.c_proj.bias
transformer.h.15.ln_2.weight
transformer.h.15.ln_2.bias
transformer.h.15.mlp.c_fc.weight
transformer.h.15.mlp.c_fc.bias
transformer.h.15.mlp.c_proj.weight
transformer.h.15.mlp.c_proj.bias
transformer.h.16.prefix_word
transformer.h.16.ln_1.weight
transformer.h.16.ln_1.bias
transformer.h.16.attn.bias
transformer.h.16.attn.masked_bias
transformer.h.16.attn.c_attn.weight
transformer.h.16.attn.c_attn.bias
transformer.h.16.attn.c_proj.weight
transformer.h.16.attn.c_proj.bias
transformer.h.16.ln_2.weight
transformer.h.16.ln_2.bias
transformer.h.16.mlp.c_fc.weight
transformer.h.16.mlp.c_fc.bias
transformer.h.16.mlp.c_proj.weight
transformer.h.16.mlp.c_proj.bias
transformer.h.17.prefix_word
transformer.h.17.ln_1.weight
transformer.h.17.ln_1.bias
transformer.h.17.attn.bias
transformer.h.17.attn.masked_bias
transformer.h.17.attn.c_attn.weight
transformer.h.17.attn.c_attn.bias
transformer.h.17.attn.c_proj.weight
transformer.h.17.attn.c_proj.bias
transformer.h.17.ln_2.weight
transformer.h.17.ln_2.bias
transformer.h.17.mlp.c_fc.weight
transformer.h.17.mlp.c_fc.bias
transformer.h.17.mlp.c_proj.weight
transformer.h.17.mlp.c_proj.bias
transformer.h.18.prefix_word
transformer.h.18.ln_1.weight
transformer.h.18.ln_1.bias
transformer.h.18.attn.bias
transformer.h.18.attn.masked_bias
transformer.h.18.attn.c_attn.weight
transformer.h.18.attn.c_attn.bias
transformer.h.18.attn.c_proj.weight
transformer.h.18.attn.c_proj.bias
transformer.h.18.ln_2.weight
transformer.h.18.ln_2.bias
transformer.h.18.mlp.c_fc.weight
transformer.h.18.mlp.c_fc.bias
transformer.h.18.mlp.c_proj.weight
transformer.h.18.mlp.c_proj.bias
transformer.h.19.prefix_word
transformer.h.19.ln_1.weight
transformer.h.19.ln_1.bias
transformer.h.19.attn.bias
transformer.h.19.attn.masked_bias
transformer.h.19.attn.c_attn.weight
transformer.h.19.attn.c_attn.bias
transformer.h.19.attn.c_proj.weight
transformer.h.19.attn.c_proj.bias
transformer.h.19.ln_2.weight
transformer.h.19.ln_2.bias
transformer.h.19.mlp.c_fc.weight
transformer.h.19.mlp.c_fc.bias
transformer.h.19.mlp.c_proj.weight
transformer.h.19.mlp.c_proj.bias
transformer.h.20.prefix_word
transformer.h.20.ln_1.weight
transformer.h.20.ln_1.bias
transformer.h.20.attn.bias
transformer.h.20.attn.masked_bias
transformer.h.20.attn.c_attn.weight
transformer.h.20.attn.c_attn.bias
transformer.h.20.attn.c_proj.weight
transformer.h.20.attn.c_proj.bias
transformer.h.20.ln_2.weight
transformer.h.20.ln_2.bias
transformer.h.20.mlp.c_fc.weight
transformer.h.20.mlp.c_fc.bias
transformer.h.20.mlp.c_proj.weight
transformer.h.20.mlp.c_proj.bias
transformer.h.21.prefix_word
transformer.h.21.ln_1.weight
transformer.h.21.ln_1.bias
transformer.h.21.attn.bias
transformer.h.21.attn.masked_bias
transformer.h.21.attn.c_attn.weight
transformer.h.21.attn.c_attn.bias
transformer.h.21.attn.c_proj.weight
transformer.h.21.attn.c_proj.bias
transformer.h.21.ln_2.weight
transformer.h.21.ln_2.bias
transformer.h.21.mlp.c_fc.weight
transformer.h.21.mlp.c_fc.bias
transformer.h.21.mlp.c_proj.weight
transformer.h.21.mlp.c_proj.bias
transformer.h.22.prefix_word
transformer.h.22.ln_1.weight
transformer.h.22.ln_1.bias
transformer.h.22.attn.bias
transformer.h.22.attn.masked_bias
transformer.h.22.attn.c_attn.weight
transformer.h.22.attn.c_attn.bias
transformer.h.22.attn.c_proj.weight
transformer.h.22.attn.c_proj.bias
transformer.h.22.ln_2.weight
transformer.h.22.ln_2.bias
transformer.h.22.mlp.c_fc.weight
transformer.h.22.mlp.c_fc.bias
transformer.h.22.mlp.c_proj.weight
transformer.h.22.mlp.c_proj.bias
transformer.h.23.prefix_word
transformer.h.23.ln_1.weight
transformer.h.23.ln_1.bias
transformer.h.23.attn.bias
transformer.h.23.attn.masked_bias
transformer.h.23.attn.c_attn.weight
transformer.h.23.attn.c_attn.bias
transformer.h.23.attn.c_proj.weight
transformer.h.23.attn.c_proj.bias
transformer.h.23.ln_2.weight
transformer.h.23.ln_2.bias
transformer.h.23.mlp.c_fc.weight
transformer.h.23.mlp.c_fc.bias
transformer.h.23.mlp.c_proj.weight
transformer.h.23.mlp.c_proj.bias
transformer.h.24.prefix_word
transformer.h.24.ln_1.weight
transformer.h.24.ln_1.bias
transformer.h.24.attn.bias
transformer.h.24.attn.masked_bias
transformer.h.24.attn.c_attn.weight
transformer.h.24.attn.c_attn.bias
transformer.h.24.attn.c_proj.weight
transformer.h.24.attn.c_proj.bias
transformer.h.24.ln_2.weight
transformer.h.24.ln_2.bias
transformer.h.24.mlp.c_fc.weight
transformer.h.24.mlp.c_fc.bias
transformer.h.24.mlp.c_proj.weight
transformer.h.24.mlp.c_proj.bias
transformer.h.25.prefix_word
transformer.h.25.ln_1.weight
transformer.h.25.ln_1.bias
transformer.h.25.attn.bias
transformer.h.25.attn.masked_bias
transformer.h.25.attn.c_attn.weight
transformer.h.25.attn.c_attn.bias
transformer.h.25.attn.c_proj.weight
transformer.h.25.attn.c_proj.bias
transformer.h.25.ln_2.weight
transformer.h.25.ln_2.bias
transformer.h.25.mlp.c_fc.weight
transformer.h.25.mlp.c_fc.bias
transformer.h.25.mlp.c_proj.weight
transformer.h.25.mlp.c_proj.bias
transformer.h.26.prefix_word
transformer.h.26.ln_1.weight
transformer.h.26.ln_1.bias
transformer.h.26.attn.bias
transformer.h.26.attn.masked_bias
transformer.h.26.attn.c_attn.weight
transformer.h.26.attn.c_attn.bias
transformer.h.26.attn.c_proj.weight
transformer.h.26.attn.c_proj.bias
transformer.h.26.ln_2.weight
transformer.h.26.ln_2.bias
transformer.h.26.mlp.c_fc.weight
transformer.h.26.mlp.c_fc.bias
transformer.h.26.mlp.c_proj.weight
transformer.h.26.mlp.c_proj.bias
transformer.h.27.prefix_word
transformer.h.27.ln_1.weight
transformer.h.27.ln_1.bias
transformer.h.27.attn.bias
transformer.h.27.attn.masked_bias
transformer.h.27.attn.c_attn.weight
transformer.h.27.attn.c_attn.bias
transformer.h.27.attn.c_proj.weight
transformer.h.27.attn.c_proj.bias
transformer.h.27.ln_2.weight
transformer.h.27.ln_2.bias
transformer.h.27.mlp.c_fc.weight
transformer.h.27.mlp.c_fc.bias
transformer.h.27.mlp.c_proj.weight
transformer.h.27.mlp.c_proj.bias
transformer.h.28.prefix_word
transformer.h.28.ln_1.weight
transformer.h.28.ln_1.bias
transformer.h.28.attn.bias
transformer.h.28.attn.masked_bias
transformer.h.28.attn.c_attn.weight
transformer.h.28.attn.c_attn.bias
transformer.h.28.attn.c_proj.weight
transformer.h.28.attn.c_proj.bias
transformer.h.28.ln_2.weight
transformer.h.28.ln_2.bias
transformer.h.28.mlp.c_fc.weight
transformer.h.28.mlp.c_fc.bias
transformer.h.28.mlp.c_proj.weight
transformer.h.28.mlp.c_proj.bias
transformer.h.29.prefix_word
transformer.h.29.ln_1.weight
transformer.h.29.ln_1.bias
transformer.h.29.attn.bias
transformer.h.29.attn.masked_bias
transformer.h.29.attn.c_attn.weight
transformer.h.29.attn.c_attn.bias
transformer.h.29.attn.c_proj.weight
transformer.h.29.attn.c_proj.bias
transformer.h.29.ln_2.weight
transformer.h.29.ln_2.bias
transformer.h.29.mlp.c_fc.weight
transformer.h.29.mlp.c_fc.bias
transformer.h.29.mlp.c_proj.weight
transformer.h.29.mlp.c_proj.bias
transformer.h.30.prefix_word
transformer.h.30.ln_1.weight
transformer.h.30.ln_1.bias
transformer.h.30.attn.bias
transformer.h.30.attn.masked_bias
transformer.h.30.attn.c_attn.weight
transformer.h.30.attn.c_attn.bias
transformer.h.30.attn.c_proj.weight
transformer.h.30.attn.c_proj.bias
transformer.h.30.ln_2.weight
transformer.h.30.ln_2.bias
transformer.h.30.mlp.c_fc.weight
transformer.h.30.mlp.c_fc.bias
transformer.h.30.mlp.c_proj.weight
transformer.h.30.mlp.c_proj.bias
transformer.h.31.prefix_word
transformer.h.31.ln_1.weight
transformer.h.31.ln_1.bias
transformer.h.31.attn.bias
transformer.h.31.attn.masked_bias
transformer.h.31.attn.c_attn.weight
transformer.h.31.attn.c_attn.bias
transformer.h.31.attn.c_proj.weight
transformer.h.31.attn.c_proj.bias
transformer.h.31.ln_2.weight
transformer.h.31.ln_2.bias
transformer.h.31.mlp.c_fc.weight
transformer.h.31.mlp.c_fc.bias
transformer.h.31.mlp.c_proj.weight
transformer.h.31.mlp.c_proj.bias
transformer.h.32.prefix_word
transformer.h.32.ln_1.weight
transformer.h.32.ln_1.bias
transformer.h.32.attn.bias
transformer.h.32.attn.masked_bias
transformer.h.32.attn.c_attn.weight
transformer.h.32.attn.c_attn.bias
transformer.h.32.attn.c_proj.weight
transformer.h.32.attn.c_proj.bias
transformer.h.32.ln_2.weight
transformer.h.32.ln_2.bias
transformer.h.32.mlp.c_fc.weight
transformer.h.32.mlp.c_fc.bias
transformer.h.32.mlp.c_proj.weight
transformer.h.32.mlp.c_proj.bias
transformer.h.33.prefix_word
transformer.h.33.ln_1.weight
transformer.h.33.ln_1.bias
transformer.h.33.attn.bias
transformer.h.33.attn.masked_bias
transformer.h.33.attn.c_attn.weight
transformer.h.33.attn.c_attn.bias
transformer.h.33.attn.c_proj.weight
transformer.h.33.attn.c_proj.bias
transformer.h.33.ln_2.weight
transformer.h.33.ln_2.bias
transformer.h.33.mlp.c_fc.weight
transformer.h.33.mlp.c_fc.bias
transformer.h.33.mlp.c_proj.weight
transformer.h.33.mlp.c_proj.bias
transformer.h.34.prefix_word
transformer.h.34.ln_1.weight
transformer.h.34.ln_1.bias
transformer.h.34.attn.bias
transformer.h.34.attn.masked_bias
transformer.h.34.attn.c_attn.weight
transformer.h.34.attn.c_attn.bias
transformer.h.34.attn.c_proj.weight
transformer.h.34.attn.c_proj.bias
transformer.h.34.ln_2.weight
transformer.h.34.ln_2.bias
transformer.h.34.mlp.c_fc.weight
transformer.h.34.mlp.c_fc.bias
transformer.h.34.mlp.c_proj.weight
transformer.h.34.mlp.c_proj.bias
transformer.h.35.prefix_word
transformer.h.35.ln_1.weight
transformer.h.35.ln_1.bias
transformer.h.35.attn.bias
transformer.h.35.attn.masked_bias
transformer.h.35.attn.c_attn.weight
transformer.h.35.attn.c_attn.bias
transformer.h.35.attn.c_proj.weight
transformer.h.35.attn.c_proj.bias
transformer.h.35.ln_2.weight
transformer.h.35.ln_2.bias
transformer.h.35.mlp.c_fc.weight
transformer.h.35.mlp.c_fc.bias
transformer.h.35.mlp.c_proj.weight
transformer.h.35.mlp.c_proj.bias
transformer.h.36.prefix_word
transformer.h.36.ln_1.weight
transformer.h.36.ln_1.bias
transformer.h.36.attn.bias
transformer.h.36.attn.masked_bias
transformer.h.36.attn.c_attn.weight
transformer.h.36.attn.c_attn.bias
transformer.h.36.attn.c_proj.weight
transformer.h.36.attn.c_proj.bias
transformer.h.36.ln_2.weight
transformer.h.36.ln_2.bias
transformer.h.36.mlp.c_fc.weight
transformer.h.36.mlp.c_fc.bias
transformer.h.36.mlp.c_proj.weight
transformer.h.36.mlp.c_proj.bias
transformer.h.37.prefix_word
transformer.h.37.ln_1.weight
transformer.h.37.ln_1.bias
transformer.h.37.attn.bias
transformer.h.37.attn.masked_bias
transformer.h.37.attn.c_attn.weight
transformer.h.37.attn.c_attn.bias
transformer.h.37.attn.c_proj.weight
transformer.h.37.attn.c_proj.bias
transformer.h.37.ln_2.weight
transformer.h.37.ln_2.bias
transformer.h.37.mlp.c_fc.weight
transformer.h.37.mlp.c_fc.bias
transformer.h.37.mlp.c_proj.weight
transformer.h.37.mlp.c_proj.bias
transformer.h.38.prefix_word
transformer.h.38.ln_1.weight
transformer.h.38.ln_1.bias
transformer.h.38.attn.bias
transformer.h.38.attn.masked_bias
transformer.h.38.attn.c_attn.weight
transformer.h.38.attn.c_attn.bias
transformer.h.38.attn.c_proj.weight
transformer.h.38.attn.c_proj.bias
transformer.h.38.ln_2.weight
transformer.h.38.ln_2.bias
transformer.h.38.mlp.c_fc.weight
transformer.h.38.mlp.c_fc.bias
transformer.h.38.mlp.c_proj.weight
transformer.h.38.mlp.c_proj.bias
transformer.h.39.prefix_word
transformer.h.39.ln_1.weight
transformer.h.39.ln_1.bias
transformer.h.39.attn.bias
transformer.h.39.attn.masked_bias
transformer.h.39.attn.c_attn.weight
transformer.h.39.attn.c_attn.bias
transformer.h.39.attn.c_proj.weight
transformer.h.39.attn.c_proj.bias
transformer.h.39.ln_2.weight
transformer.h.39.ln_2.bias
transformer.h.39.mlp.c_fc.weight
transformer.h.39.mlp.c_fc.bias
transformer.h.39.mlp.c_proj.weight
transformer.h.39.mlp.c_proj.bias
transformer.h.40.prefix_word
transformer.h.40.ln_1.weight
transformer.h.40.ln_1.bias
transformer.h.40.attn.bias
transformer.h.40.attn.masked_bias
transformer.h.40.attn.c_attn.weight
transformer.h.40.attn.c_attn.bias
transformer.h.40.attn.c_proj.weight
transformer.h.40.attn.c_proj.bias
transformer.h.40.ln_2.weight
transformer.h.40.ln_2.bias
transformer.h.40.mlp.c_fc.weight
transformer.h.40.mlp.c_fc.bias
transformer.h.40.mlp.c_proj.weight
transformer.h.40.mlp.c_proj.bias
transformer.h.41.prefix_word
transformer.h.41.ln_1.weight
transformer.h.41.ln_1.bias
transformer.h.41.attn.bias
transformer.h.41.attn.masked_bias
transformer.h.41.attn.c_attn.weight
transformer.h.41.attn.c_attn.bias
transformer.h.41.attn.c_proj.weight
transformer.h.41.attn.c_proj.bias
transformer.h.41.ln_2.weight
transformer.h.41.ln_2.bias
transformer.h.41.mlp.c_fc.weight
transformer.h.41.mlp.c_fc.bias
transformer.h.41.mlp.c_proj.weight
transformer.h.41.mlp.c_proj.bias
transformer.h.42.prefix_word
transformer.h.42.ln_1.weight
transformer.h.42.ln_1.bias
transformer.h.42.attn.bias
transformer.h.42.attn.masked_bias
transformer.h.42.attn.c_attn.weight
transformer.h.42.attn.c_attn.bias
transformer.h.42.attn.c_proj.weight
transformer.h.42.attn.c_proj.bias
transformer.h.42.ln_2.weight
transformer.h.42.ln_2.bias
transformer.h.42.mlp.c_fc.weight
transformer.h.42.mlp.c_fc.bias
transformer.h.42.mlp.c_proj.weight
transformer.h.42.mlp.c_proj.bias
transformer.h.43.prefix_word
transformer.h.43.ln_1.weight
transformer.h.43.ln_1.bias
transformer.h.43.attn.bias
transformer.h.43.attn.masked_bias
transformer.h.43.attn.c_attn.weight
transformer.h.43.attn.c_attn.bias
transformer.h.43.attn.c_proj.weight
transformer.h.43.attn.c_proj.bias
transformer.h.43.ln_2.weight
transformer.h.43.ln_2.bias
transformer.h.43.mlp.c_fc.weight
transformer.h.43.mlp.c_fc.bias
transformer.h.43.mlp.c_proj.weight
transformer.h.43.mlp.c_proj.bias
transformer.h.44.prefix_word
transformer.h.44.ln_1.weight
transformer.h.44.ln_1.bias
transformer.h.44.attn.bias
transformer.h.44.attn.masked_bias
transformer.h.44.attn.c_attn.weight
transformer.h.44.attn.c_attn.bias
transformer.h.44.attn.c_proj.weight
transformer.h.44.attn.c_proj.bias
transformer.h.44.ln_2.weight
transformer.h.44.ln_2.bias
transformer.h.44.mlp.c_fc.weight
transformer.h.44.mlp.c_fc.bias
transformer.h.44.mlp.c_proj.weight
transformer.h.44.mlp.c_proj.bias
transformer.h.45.prefix_word
transformer.h.45.ln_1.weight
transformer.h.45.ln_1.bias
transformer.h.45.attn.bias
transformer.h.45.attn.masked_bias
transformer.h.45.attn.c_attn.weight
transformer.h.45.attn.c_attn.bias
transformer.h.45.attn.c_proj.weight
transformer.h.45.attn.c_proj.bias
transformer.h.45.ln_2.weight
transformer.h.45.ln_2.bias
transformer.h.45.mlp.c_fc.weight
transformer.h.45.mlp.c_fc.bias
transformer.h.45.mlp.c_proj.weight
transformer.h.45.mlp.c_proj.bias
transformer.h.46.prefix_word
transformer.h.46.ln_1.weight
transformer.h.46.ln_1.bias
transformer.h.46.attn.bias
transformer.h.46.attn.masked_bias
transformer.h.46.attn.c_attn.weight
transformer.h.46.attn.c_attn.bias
transformer.h.46.attn.c_proj.weight
transformer.h.46.attn.c_proj.bias
transformer.h.46.ln_2.weight
transformer.h.46.ln_2.bias
transformer.h.46.mlp.c_fc.weight
transformer.h.46.mlp.c_fc.bias
transformer.h.46.mlp.c_proj.weight
transformer.h.46.mlp.c_proj.bias
transformer.h.47.prefix_word
transformer.h.47.ln_1.weight
transformer.h.47.ln_1.bias
transformer.h.47.attn.bias
transformer.h.47.attn.masked_bias
transformer.h.47.attn.c_attn.weight
transformer.h.47.attn.c_attn.bias
transformer.h.47.attn.c_proj.weight
transformer.h.47.attn.c_proj.bias
transformer.h.47.ln_2.weight
transformer.h.47.ln_2.bias
transformer.h.47.mlp.c_fc.weight
transformer.h.47.mlp.c_fc.bias
transformer.h.47.mlp.c_proj.weight
transformer.h.47.mlp.c_proj.bias
transformer.ln_f.weight
transformer.ln_f.bias
lm_head.weight
        test_set, perplexity 131.89
