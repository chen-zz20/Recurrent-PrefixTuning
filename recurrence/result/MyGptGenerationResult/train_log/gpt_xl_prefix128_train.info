[38;5;2m[i 0109 09:31:58.328559 24 compiler.py:955] Jittor(1.3.6.10) src: /home/chenzz/.conda/envs/HW3/lib/python3.7/site-packages/jittor[m
[38;5;2m[i 0109 09:31:58.331616 24 compiler.py:956] g++ at /usr/bin/g++(7.5.0)[m
[38;5;2m[i 0109 09:31:58.331683 24 compiler.py:957] cache_path: /home/chenzz/.cache/jittor/jt1.3.6/g++7.5.0/py3.7.6/Linux-5.3.18-5x95/AMDRyzen73700Xx25/default[m
[38;5;2m[i 0109 09:31:59.166882 24 install_cuda.py:93] cuda_driver_version: [11, 3][m
[38;5;2m[i 0109 09:31:59.167199 24 install_cuda.py:81] restart /home/chenzz/.conda/envs/HW3/bin/python ['main.py', '--name', 'gpt-xl-prefix128', '--num_epochs', '30', '--prefix', '128', '--model_config', 'config_xl.json', '--pretrain_dir', 'pretrain/gpt2-xl-paras.jt', '--batch_size', '16'][m
[38;5;2m[i 0109 09:31:59.342963 68 compiler.py:955] Jittor(1.3.6.10) src: /home/chenzz/.conda/envs/HW3/lib/python3.7/site-packages/jittor[m
[38;5;2m[i 0109 09:31:59.346112 68 compiler.py:956] g++ at /usr/bin/g++(7.5.0)[m
[38;5;2m[i 0109 09:31:59.346224 68 compiler.py:957] cache_path: /home/chenzz/.cache/jittor/jt1.3.6/g++7.5.0/py3.7.6/Linux-5.3.18-5x95/AMDRyzen73700Xx25/default[m
[38;5;2m[i 0109 09:32:00.196360 68 install_cuda.py:93] cuda_driver_version: [11, 3][m
[38;5;2m[i 0109 09:32:00.199720 68 __init__.py:411] Found /home/chenzz/.cache/jittor/jtcuda/cuda11.2_cudnn8_linux/bin/nvcc(11.2.152) at /home/chenzz/.cache/jittor/jtcuda/cuda11.2_cudnn8_linux/bin/nvcc.[m
[38;5;2m[i 0109 09:32:00.204167 68 __init__.py:411] Found addr2line(2.35.1) at /usr/bin/addr2line.[m
[38;5;2m[i 0109 09:32:01.163740 68 compiler.py:1010] cuda key:cu11.2.152_sm_86[m
[38;5;2m[i 0109 09:32:01.329987 68 __init__.py:227] Total mem: 125.72GB, using 16 procs for compiling.[m
[38;5;2m[i 0109 09:32:01.415745 68 jit_compiler.cc:28] Load cc_path: /usr/bin/g++[m
[38;5;2m[i 0109 09:32:02.239765 68 init.cc:62] Found cuda archs: [86,][m
[38;5;2m[i 0109 09:32:02.259773 68 compile_extern.py:522] mpicc not found, distribution disabled.[m
[38;5;2m[i 0109 09:32:03.246800 68 cuda_flags.cc:39] CUDA enabled.[m
Namespace(batch_size=16, cpu_count=1, data_dir='./data', decode_strategy='random', learning_rate=0.0001, maxlen=35, model_config='config_xl.json', name='gpt-xl-prefix128', num_epochs=30, prefix=128, pretrain_dir='pretrain/gpt2-xl-paras.jt', temperature=1, test=None, tokenizer_dir='./tokenizer', top_k=40, top_p=1.0, train_dir='./train_test')
Tokenizer PAD ID: 50256
Loading Data ...
49
[128,1600,]
[128,1600,]
[128,1600,]
[128,1600,]
[128,1600,]
[128,1600,]
[128,1600,]
[128,1600,]
[128,1600,]
[128,1600,]
[128,1600,]
[128,1600,]
[128,1600,]
[128,1600,]
[128,1600,]
[128,1600,]
[128,1600,]
[128,1600,]
[128,1600,]
[128,1600,]
[128,1600,]
[128,1600,]
[128,1600,]
[128,1600,]
[128,1600,]
[128,1600,]
[128,1600,]
[128,1600,]
[128,1600,]
[128,1600,]
[128,1600,]
[128,1600,]
[128,1600,]
[128,1600,]
[128,1600,]
[128,1600,]
[128,1600,]
[128,1600,]
[128,1600,]
[128,1600,]
[128,1600,]
[128,1600,]
[128,1600,]
[128,1600,]
[128,1600,]
[128,1600,]
[128,1600,]
[128,1600,]
[50257,1600,]
jittor_model info:
TfmrLMHeadModel(
    transformer: TfmrModel(
        wte: Embedding(50257, 1600)
        wpe: Embedding(1024, 1600)
        drop: Dropout(0.1, is_train=False)
        h: Sequential(
            0: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            1: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            2: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            3: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            4: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            5: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            6: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            7: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            8: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            9: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            10: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            11: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            12: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            13: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            14: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            15: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            16: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            17: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            18: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            19: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            20: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            21: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            22: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            23: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            24: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            25: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            26: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            27: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            28: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            29: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            30: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            31: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            32: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            33: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            34: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            35: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            36: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            37: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            38: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            39: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            40: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            41: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            42: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            43: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            44: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            45: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            46: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            47: TfmrBlock(
                ln_1: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(4800, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1600,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(6400, None, None)
                    c_proj: TransposeLinear(1600, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
        )
        ln_f: LayerNorm((1600,), 1e-05, elementwise_affine=True)
    )
    lm_head: Linear(1600, 50257, None, None)
)
Epoch 1 Batch 10, train loss 9.763850
Epoch 1 Batch 20, train loss 8.719730
Epoch 1 Batch 30, train loss 7.962886
Epoch 1 Batch 40, train loss 7.367651
Epoch 1 Batch 50, train loss 6.980154
Epoch 1 Batch 60, train loss 6.699129
Epoch 1 Batch 70, train loss 6.476254
Epoch 1 Batch 80, train loss 6.292970
Epoch 1 Batch 90, train loss 6.137954
Epoch 1 Batch 100, train loss 6.023487
Epoch 1 Batch 110, train loss 5.543023
Epoch 1 Batch 120, train loss 5.275459
Epoch 1 Batch 130, train loss 5.133761
Epoch 1 Batch 140, train loss 5.076270
Epoch 1 Batch 150, train loss 5.037755
Epoch 1 Batch 160, train loss 5.019909
Epoch 1 Batch 170, train loss 5.057817
Epoch 1 Batch 180, train loss 5.091326
Epoch 1 Batch 190, train loss 5.149045
Epoch 1 Batch 200, train loss 5.194608
Epoch 1 Batch 210, train loss 5.237003
Epoch 1 Batch 220, train loss 5.263910
Epoch 1 Batch 230, train loss 5.291385
Epoch 1 Batch 240, train loss 5.293300
Epoch 1 Batch 250, train loss 5.282632
Epoch 1 Batch 260, train loss 5.263876
Epoch 1 Batch 270, train loss 5.192596
Epoch 1 Batch 280, train loss 5.121992
Epoch 1 Batch 290, train loss 5.065915
Epoch 1 Batch 300, train loss 4.996939
Epoch 1 Batch 310, train loss 4.933605
Epoch 1 Batch 320, train loss 4.883886
Epoch 1 Batch 330, train loss 4.835795
Epoch 1 Batch 340, train loss 4.822487
Epoch 1 Batch 350, train loss 4.806882
Epoch 1 Batch 360, train loss 4.789141
Epoch 1 Batch 370, train loss 4.784384
Epoch 1 Batch 380, train loss 4.806869
Epoch 1 Batch 390, train loss 4.789223
Epoch 1 Batch 400, train loss 4.783059
Epoch 1 Batch 410, train loss 4.773003
Epoch 1 Batch 420, train loss 4.764881
Epoch 1 Batch 430, train loss 4.746014
Epoch 1 Batch 440, train loss 4.721951
Epoch 1 Batch 450, train loss 4.708368
Epoch 1 Batch 460, train loss 4.678384
Epoch 1 Batch 470, train loss 4.651499
Epoch 1 Batch 480, train loss 4.613079
Epoch 1 Batch 490, train loss 4.593592
Epoch 1 Batch 500, train loss 4.576740
Epoch 1 Batch 510, train loss 4.571955
Epoch 1 Batch 520, train loss 4.555425
Epoch 1 Batch 530, train loss 4.521007
Epoch 1 Batch 540, train loss 4.497505
Epoch 1 Batch 550, train loss 4.488093
Epoch 1 Batch 560, train loss 4.495865
Epoch 1 Batch 570, train loss 4.488365
Epoch 1 Batch 580, train loss 4.499573
Epoch 1 Batch 590, train loss 4.489470
Epoch 1 Batch 600, train loss 4.503610
Epoch 1 Batch 610, train loss 4.493964
Epoch 1 Batch 620, train loss 4.492993
Epoch 1 Batch 630, train loss 4.517436
Epoch 1 Batch 640, train loss 4.506605
Epoch 1 Batch 650, train loss 4.494423
Epoch 1 Batch 660, train loss 4.476796
Epoch 1 Batch 670, train loss 4.472714
Epoch 1 Batch 680, train loss 4.457541
Epoch 1 Batch 690, train loss 4.434648
Epoch 1 Batch 700, train loss 4.384056
Epoch 1 Batch 710, train loss 4.343404
Epoch 1 Batch 720, train loss 4.337146
Epoch 1 Batch 730, train loss 4.315170
Epoch 1 Batch 740, train loss 4.314383
Epoch 1 Batch 750, train loss 4.303417
Epoch 1 Batch 760, train loss 4.280200
Epoch 1 Batch 770, train loss 4.257404
Epoch 1 Batch 780, train loss 4.242591
Epoch 1 Batch 790, train loss 4.228719
Epoch 1 Batch 800, train loss 4.222265
Epoch 1 Batch 810, train loss 4.214746
Epoch 1 Batch 820, train loss 4.189441
Epoch 1 Batch 830, train loss 4.166529
Epoch 1 Batch 840, train loss 4.146440
Epoch 1 Batch 850, train loss 4.108959
Epoch 1 Batch 860, train loss 4.096665
Epoch 1 Batch 870, train loss 4.079610
Epoch 1 Batch 880, train loss 4.050027
Epoch 1 Batch 890, train loss 4.035776
Epoch 1 Batch 900, train loss 4.019900
Epoch 1 Batch 910, train loss 3.999941
Epoch 1 Batch 920, train loss 3.976334
Epoch 1 Batch 930, train loss 3.979525
Epoch 1 of 30 took 1805.1684432029724s
  training loss:                 4.717885088564745
  validation loss:               5.144261611318588
  validation perplexity:         171.44484498913573
  best epoch:                    1
  best validation perplexity:    171.44484498913573
Epoch 2 Batch 10, train loss 4.923266
Epoch 2 Batch 20, train loss 4.933224
Epoch 2 Batch 30, train loss 4.922674
Epoch 2 Batch 40, train loss 4.949582
Epoch 2 Batch 50, train loss 4.998146
Epoch 2 Batch 60, train loss 5.005816
Epoch 2 Batch 70, train loss 5.028293
Epoch 2 Batch 80, train loss 5.031573
Epoch 2 Batch 90, train loss 5.029692
Epoch 2 Batch 100, train loss 5.024430
Epoch 2 Batch 110, train loss 5.044467
Epoch 2 Batch 120, train loss 5.056050
Epoch 2 Batch 130, train loss 5.070911
Epoch 2 Batch 140, train loss 5.077296
Epoch 2 Batch 150, train loss 5.065998
Epoch 2 Batch 160, train loss 5.069850
Epoch 2 Batch 170, train loss 5.052823
Epoch 2 Batch 180, train loss 5.036917
Epoch 2 Batch 190, train loss 5.019675
Epoch 2 Batch 200, train loss 5.018494
Epoch 2 Batch 210, train loss 4.994850
Epoch 2 Batch 220, train loss 4.985615
Epoch 2 Batch 230, train loss 4.968205
Epoch 2 Batch 240, train loss 4.933486
Epoch 2 Batch 250, train loss 4.905696
Epoch 2 Batch 260, train loss 4.885262
Epoch 2 Batch 270, train loss 4.864569
Epoch 2 Batch 280, train loss 4.840935
Epoch 2 Batch 290, train loss 4.849465
Epoch 2 Batch 300, train loss 4.840756
Epoch 2 Batch 310, train loss 4.831263
Epoch 2 Batch 320, train loss 4.816270
Epoch 2 Batch 330, train loss 4.808168
Epoch 2 Batch 340, train loss 4.815218
Epoch 2 Batch 350, train loss 4.815415
Epoch 2 Batch 360, train loss 4.813336
Epoch 2 Batch 370, train loss 4.815835
Epoch 2 Batch 380, train loss 4.834704
Epoch 2 Batch 390, train loss 4.826051
Epoch 2 Batch 400, train loss 4.812455
Epoch 2 Batch 410, train loss 4.815075
Epoch 2 Batch 420, train loss 4.817146
Epoch 2 Batch 430, train loss 4.820291
Epoch 2 Batch 440, train loss 4.824404
Epoch 2 Batch 450, train loss 4.821903
Epoch 2 Batch 460, train loss 4.808332
Epoch 2 Batch 470, train loss 4.806706
Epoch 2 Batch 480, train loss 4.803546
Epoch 2 Batch 490, train loss 4.803381
Epoch 2 Batch 500, train loss 4.812360
Epoch 2 Batch 510, train loss 4.812835
Epoch 2 Batch 520, train loss 4.806052
Epoch 2 Batch 530, train loss 4.784801
Epoch 2 Batch 540, train loss 4.773647
Epoch 2 Batch 550, train loss 4.782343
Epoch 2 Batch 560, train loss 4.792197
Epoch 2 Batch 570, train loss 4.791649
Epoch 2 Batch 580, train loss 4.799304
Epoch 2 Batch 590, train loss 4.791718
Epoch 2 Batch 600, train loss 4.763622
Epoch 2 Batch 610, train loss 4.720374
Epoch 2 Batch 620, train loss 4.690402
Epoch 2 Batch 630, train loss 4.678772
Epoch 2 Batch 640, train loss 4.638154
Epoch 2 Batch 650, train loss 4.597560
Epoch 2 Batch 660, train loss 4.557769
Epoch 2 Batch 670, train loss 4.525766
Epoch 2 Batch 680, train loss 4.489137
Epoch 2 Batch 690, train loss 4.449604
Epoch 2 Batch 700, train loss 4.429501
Epoch 2 Batch 710, train loss 4.419408
Epoch 2 Batch 720, train loss 4.425417
Epoch 2 Batch 730, train loss 4.424497
Epoch 2 Batch 740, train loss 4.435233
Epoch 2 Batch 750, train loss 4.435939
Epoch 2 Batch 760, train loss 4.422660
Epoch 2 Batch 770, train loss 4.408970
Epoch 2 Batch 780, train loss 4.406424
Epoch 2 Batch 790, train loss 4.399093
Epoch 2 Batch 800, train loss 4.398425
Epoch 2 Batch 810, train loss 4.396490
Epoch 2 Batch 820, train loss 4.378524
Epoch 2 Batch 830, train loss 4.361721
Epoch 2 Batch 840, train loss 4.355870
Epoch 2 Batch 850, train loss 4.332727
Epoch 2 Batch 860, train loss 4.339166
Epoch 2 Batch 870, train loss 4.337184
Epoch 2 Batch 880, train loss 4.321424
Epoch 2 Batch 890, train loss 4.325682
Epoch 2 Batch 900, train loss 4.323898
Epoch 2 Batch 910, train loss 4.316523
Epoch 2 Batch 920, train loss 4.311176
Epoch 2 Batch 930, train loss 4.313649
Epoch 2 of 30 took 1032.1648879051208s
  training loss:                 4.697945655790219
  validation loss:               5.144209504508972
  validation perplexity:         171.4359117779809
  best epoch:                    2
  best validation perplexity:    171.4359117779809
Epoch 3 Batch 10, train loss 4.924118
Epoch 3 Batch 20, train loss 4.932728
Epoch 3 Batch 30, train loss 4.922520
Epoch 3 Batch 40, train loss 4.949738
Epoch 3 Batch 50, train loss 4.998614
Epoch 3 Batch 60, train loss 5.006184
Epoch 3 Batch 70, train loss 5.028717
Epoch 3 Batch 80, train loss 5.031863
Epoch 3 Batch 90, train loss 5.030033
Epoch 3 Batch 100, train loss 5.024750
Epoch 3 Batch 110, train loss 5.044907
Epoch 3 Batch 120, train loss 5.056665
Epoch 3 Batch 130, train loss 5.071468
Epoch 3 Batch 140, train loss 5.077785
Epoch 3 Batch 150, train loss 5.066169
Epoch 3 Batch 160, train loss 5.069940
Epoch 3 Batch 170, train loss 5.052763
Epoch 3 Batch 180, train loss 5.036954
Epoch 3 Batch 190, train loss 5.019670
Epoch 3 Batch 200, train loss 5.018554
Epoch 3 Batch 210, train loss 4.994628
Epoch 3 Batch 220, train loss 4.985373
Epoch 3 Batch 230, train loss 4.968033
Epoch 3 Batch 240, train loss 4.933251
Epoch 3 Batch 250, train loss 4.905490
Epoch 3 Batch 260, train loss 4.885114
Epoch 3 Batch 270, train loss 4.864519
Epoch 3 Batch 280, train loss 4.840910
Epoch 3 Batch 290, train loss 4.849532
Epoch 3 Batch 300, train loss 4.840677
Epoch 3 Batch 310, train loss 4.831259
Epoch 3 Batch 320, train loss 4.816360
Epoch 3 Batch 330, train loss 4.808248
Epoch 3 Batch 340, train loss 4.815190
Epoch 3 Batch 350, train loss 4.815489
Epoch 3 Batch 360, train loss 4.813266
Epoch 3 Batch 370, train loss 4.815789
Epoch 3 Batch 380, train loss 4.834540
Epoch 3 Batch 390, train loss 4.825723
Epoch 3 Batch 400, train loss 4.812211
Epoch 3 Batch 410, train loss 4.814776
Epoch 3 Batch 420, train loss 4.816700
Epoch 3 Batch 430, train loss 4.819733
Epoch 3 Batch 440, train loss 4.823850
Epoch 3 Batch 450, train loss 4.821367
Epoch 3 Batch 460, train loss 4.808033
Epoch 3 Batch 470, train loss 4.806235
Epoch 3 Batch 480, train loss 4.803223
Epoch 3 Batch 490, train loss 4.803178
Epoch 3 Batch 500, train loss 4.812070
Epoch 3 Batch 510, train loss 4.812751
Epoch 3 Batch 520, train loss 4.806117
Epoch 3 Batch 530, train loss 4.784924
Epoch 3 Batch 540, train loss 4.773920
Epoch 3 Batch 550, train loss 4.782651
Epoch 3 Batch 560, train loss 4.792355
Epoch 3 Batch 570, train loss 4.791849
Epoch 3 Batch 580, train loss 4.799413
Epoch 3 Batch 590, train loss 4.791743
Epoch 3 Batch 600, train loss 4.763583
Epoch 3 Batch 610, train loss 4.720389
Epoch 3 Batch 620, train loss 4.690222
Epoch 3 Batch 630, train loss 4.678670
Epoch 3 Batch 640, train loss 4.638085
Epoch 3 Batch 650, train loss 4.597496
Epoch 3 Batch 660, train loss 4.557832
Epoch 3 Batch 670, train loss 4.526044
Epoch 3 Batch 680, train loss 4.489477
Epoch 3 Batch 690, train loss 4.449963
Epoch 3 Batch 700, train loss 4.429935
Epoch 3 Batch 710, train loss 4.419435
Epoch 3 Batch 720, train loss 4.425600
Epoch 3 Batch 730, train loss 4.424466
Epoch 3 Batch 740, train loss 4.435082
Epoch 3 Batch 750, train loss 4.435778
Epoch 3 Batch 760, train loss 4.422490
Epoch 3 Batch 770, train loss 4.408568
Epoch 3 Batch 780, train loss 4.405895
Epoch 3 Batch 790, train loss 4.398666
Epoch 3 Batch 800, train loss 4.398118
Epoch 3 Batch 810, train loss 4.396435
Epoch 3 Batch 820, train loss 4.378393
Epoch 3 Batch 830, train loss 4.361685
Epoch 3 Batch 840, train loss 4.355738
Epoch 3 Batch 850, train loss 4.332369
Epoch 3 Batch 860, train loss 4.338870
Epoch 3 Batch 870, train loss 4.337041
Epoch 3 Batch 880, train loss 4.321415
Epoch 3 Batch 890, train loss 4.325447
Epoch 3 Batch 900, train loss 4.323779
Epoch 3 Batch 910, train loss 4.316301
Epoch 3 Batch 920, train loss 4.310919
Epoch 3 Batch 930, train loss 4.313337
Validation loss: 171.479, becomes larger. Stop training.
