[38;5;2m[i 0109 09:09:26.266213 64 compiler.py:955] Jittor(1.3.6.10) src: /home/chenzz/.conda/envs/HW3/lib/python3.7/site-packages/jittor[m
[38;5;2m[i 0109 09:09:26.269340 64 compiler.py:956] g++ at /usr/bin/g++(7.5.0)[m
[38;5;2m[i 0109 09:09:26.269456 64 compiler.py:957] cache_path: /home/chenzz/.cache/jittor/jt1.3.6/g++7.5.0/py3.7.6/Linux-5.3.18-5x95/AMDRyzen73700Xx25/default[m
[38;5;2m[i 0109 09:09:27.096769 64 install_cuda.py:93] cuda_driver_version: [11, 3][m
[38;5;2m[i 0109 09:09:27.097112 64 install_cuda.py:81] restart /home/chenzz/.conda/envs/HW3/bin/python ['main.py', '--name', 'gpt-large-prefix128', '--num_epochs', '30', '--prefix', '128', '--model_config', 'config_large.json', '--pretrain_dir', 'pretrain/gpt2-large-paras.jt', '--batch_size', '16'][m
[38;5;2m[i 0109 09:09:27.242269 48 compiler.py:955] Jittor(1.3.6.10) src: /home/chenzz/.conda/envs/HW3/lib/python3.7/site-packages/jittor[m
[38;5;2m[i 0109 09:09:27.245371 48 compiler.py:956] g++ at /usr/bin/g++(7.5.0)[m
[38;5;2m[i 0109 09:09:27.245494 48 compiler.py:957] cache_path: /home/chenzz/.cache/jittor/jt1.3.6/g++7.5.0/py3.7.6/Linux-5.3.18-5x95/AMDRyzen73700Xx25/default[m
[38;5;2m[i 0109 09:09:28.030837 48 install_cuda.py:93] cuda_driver_version: [11, 3][m
[38;5;2m[i 0109 09:09:28.034086 48 __init__.py:411] Found /home/chenzz/.cache/jittor/jtcuda/cuda11.2_cudnn8_linux/bin/nvcc(11.2.152) at /home/chenzz/.cache/jittor/jtcuda/cuda11.2_cudnn8_linux/bin/nvcc.[m
[38;5;2m[i 0109 09:09:28.038226 48 __init__.py:411] Found addr2line(2.35.1) at /usr/bin/addr2line.[m
[38;5;2m[i 0109 09:09:28.981133 48 compiler.py:1010] cuda key:cu11.2.152_sm_86[m
[38;5;2m[i 0109 09:09:29.143536 48 __init__.py:227] Total mem: 125.72GB, using 16 procs for compiling.[m
[38;5;2m[i 0109 09:09:29.227786 48 jit_compiler.cc:28] Load cc_path: /usr/bin/g++[m
[38;5;2m[i 0109 09:09:30.057812 48 init.cc:62] Found cuda archs: [86,][m
[38;5;2m[i 0109 09:09:30.075694 48 compile_extern.py:522] mpicc not found, distribution disabled.[m
[38;5;2m[i 0109 09:09:31.064934 48 cuda_flags.cc:39] CUDA enabled.[m
Namespace(batch_size=16, cpu_count=1, data_dir='./data', decode_strategy='random', learning_rate=0.0001, maxlen=35, model_config='config_large.json', name='gpt-large-prefix128', num_epochs=30, prefix=128, pretrain_dir='pretrain/gpt2-large-paras.jt', temperature=1, test=None, tokenizer_dir='./tokenizer', top_k=40, top_p=1.0, train_dir='./train_test')
Tokenizer PAD ID: 50256
Loading Data ...
37
[128,1280,]
[128,1280,]
[128,1280,]
[128,1280,]
[128,1280,]
[128,1280,]
[128,1280,]
[128,1280,]
[128,1280,]
[128,1280,]
[128,1280,]
[128,1280,]
[128,1280,]
[128,1280,]
[128,1280,]
[128,1280,]
[128,1280,]
[128,1280,]
[128,1280,]
[128,1280,]
[128,1280,]
[128,1280,]
[128,1280,]
[128,1280,]
[128,1280,]
[128,1280,]
[128,1280,]
[128,1280,]
[128,1280,]
[128,1280,]
[128,1280,]
[128,1280,]
[128,1280,]
[128,1280,]
[128,1280,]
[128,1280,]
[50257,1280,]
jittor_model info:
TfmrLMHeadModel(
    transformer: TfmrModel(
        wte: Embedding(50257, 1280)
        wpe: Embedding(1024, 1280)
        drop: Dropout(0.1, is_train=False)
        h: Sequential(
            0: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            1: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            2: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            3: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            4: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            5: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            6: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            7: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            8: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            9: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            10: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            11: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            12: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            13: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            14: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            15: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            16: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            17: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            18: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            19: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            20: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            21: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            22: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            23: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            24: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            25: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            26: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            27: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            28: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            29: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            30: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            31: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            32: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            33: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            34: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            35: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
        )
        ln_f: LayerNorm((1280,), 1e-05, elementwise_affine=True)
    )
    lm_head: Linear(1280, 50257, None, None)
)
Epoch 1 Batch 10, train loss 10.561855
Epoch 1 Batch 20, train loss 10.091149
Epoch 1 Batch 30, train loss 9.494075
Epoch 1 Batch 40, train loss 8.792815
Epoch 1 Batch 50, train loss 8.218051
Epoch 1 Batch 60, train loss 7.788924
Epoch 1 Batch 70, train loss 7.454370
Epoch 1 Batch 80, train loss 7.185463
Epoch 1 Batch 90, train loss 6.963319
Epoch 1 Batch 100, train loss 6.793702
Epoch 1 Batch 110, train loss 6.259787
Epoch 1 Batch 120, train loss 5.822313
Epoch 1 Batch 130, train loss 5.515011
Epoch 1 Batch 140, train loss 5.364421
Epoch 1 Batch 150, train loss 5.279529
Epoch 1 Batch 160, train loss 5.231260
Epoch 1 Batch 170, train loss 5.252440
Epoch 1 Batch 180, train loss 5.267780
Epoch 1 Batch 190, train loss 5.293637
Epoch 1 Batch 200, train loss 5.315873
Epoch 1 Batch 210, train loss 5.333251
Epoch 1 Batch 220, train loss 5.343362
Epoch 1 Batch 230, train loss 5.355263
Epoch 1 Batch 240, train loss 5.351993
Epoch 1 Batch 250, train loss 5.356624
Epoch 1 Batch 260, train loss 5.357238
Epoch 1 Batch 270, train loss 5.301751
Epoch 1 Batch 280, train loss 5.254741
Epoch 1 Batch 290, train loss 5.230320
Epoch 1 Batch 300, train loss 5.189168
Epoch 1 Batch 310, train loss 5.154996
Epoch 1 Batch 320, train loss 5.124176
Epoch 1 Batch 330, train loss 5.090969
Epoch 1 Batch 340, train loss 5.088186
Epoch 1 Batch 350, train loss 5.078003
Epoch 1 Batch 360, train loss 5.058794
Epoch 1 Batch 370, train loss 5.048957
Epoch 1 Batch 380, train loss 5.062407
Epoch 1 Batch 390, train loss 5.040203
Epoch 1 Batch 400, train loss 5.025138
Epoch 1 Batch 410, train loss 5.012240
Epoch 1 Batch 420, train loss 5.002019
Epoch 1 Batch 430, train loss 4.991567
Epoch 1 Batch 440, train loss 4.975149
Epoch 1 Batch 450, train loss 4.970410
Epoch 1 Batch 460, train loss 4.947841
Epoch 1 Batch 470, train loss 4.929396
Epoch 1 Batch 480, train loss 4.900398
Epoch 1 Batch 490, train loss 4.888110
Epoch 1 Batch 500, train loss 4.880875
Epoch 1 Batch 510, train loss 4.880354
Epoch 1 Batch 520, train loss 4.872023
Epoch 1 Batch 530, train loss 4.848609
Epoch 1 Batch 540, train loss 4.831050
Epoch 1 Batch 550, train loss 4.823553
Epoch 1 Batch 560, train loss 4.838616
Epoch 1 Batch 570, train loss 4.834470
Epoch 1 Batch 580, train loss 4.845734
Epoch 1 Batch 590, train loss 4.838699
Epoch 1 Batch 600, train loss 4.857928
Epoch 1 Batch 610, train loss 4.857649
Epoch 1 Batch 620, train loss 4.856723
Epoch 1 Batch 630, train loss 4.877413
Epoch 1 Batch 640, train loss 4.872833
Epoch 1 Batch 650, train loss 4.871290
Epoch 1 Batch 660, train loss 4.858267
Epoch 1 Batch 670, train loss 4.866836
Epoch 1 Batch 680, train loss 4.866647
Epoch 1 Batch 690, train loss 4.856788
Epoch 1 Batch 700, train loss 4.817164
Epoch 1 Batch 710, train loss 4.779753
Epoch 1 Batch 720, train loss 4.781303
Epoch 1 Batch 730, train loss 4.769047
Epoch 1 Batch 740, train loss 4.771304
Epoch 1 Batch 750, train loss 4.755516
Epoch 1 Batch 760, train loss 4.733773
Epoch 1 Batch 770, train loss 4.706224
Epoch 1 Batch 780, train loss 4.681126
Epoch 1 Batch 790, train loss 4.666530
Epoch 1 Batch 800, train loss 4.653565
Epoch 1 Batch 810, train loss 4.654509
Epoch 1 Batch 820, train loss 4.644355
Epoch 1 Batch 830, train loss 4.631126
Epoch 1 Batch 840, train loss 4.619875
Epoch 1 Batch 850, train loss 4.605283
Epoch 1 Batch 860, train loss 4.614050
Epoch 1 Batch 870, train loss 4.612677
Epoch 1 Batch 880, train loss 4.600816
Epoch 1 Batch 890, train loss 4.603667
Epoch 1 Batch 900, train loss 4.605363
Epoch 1 Batch 910, train loss 4.593017
Epoch 1 Batch 920, train loss 4.571002
Epoch 1 Batch 930, train loss 4.575649
Epoch 1 of 30 took 853.2619760036469s
  training loss:                 5.1053870713024505
  validation loss:               5.211864843559265
  validation perplexity:         183.43581853462243
  best epoch:                    1
  best validation perplexity:    183.43581853462243
Epoch 2 Batch 10, train loss 4.958176
Epoch 2 Batch 20, train loss 4.988709
Epoch 2 Batch 30, train loss 4.985463
Epoch 2 Batch 40, train loss 5.018587
Epoch 2 Batch 50, train loss 5.062289
Epoch 2 Batch 60, train loss 5.077232
Epoch 2 Batch 70, train loss 5.097947
Epoch 2 Batch 80, train loss 5.103317
Epoch 2 Batch 90, train loss 5.103330
Epoch 2 Batch 100, train loss 5.100706
Epoch 2 Batch 110, train loss 5.123975
Epoch 2 Batch 120, train loss 5.137651
Epoch 2 Batch 130, train loss 5.153630
Epoch 2 Batch 140, train loss 5.160933
Epoch 2 Batch 150, train loss 5.150857
Epoch 2 Batch 160, train loss 5.151680
Epoch 2 Batch 170, train loss 5.142287
Epoch 2 Batch 180, train loss 5.131579
Epoch 2 Batch 190, train loss 5.121701
Epoch 2 Batch 200, train loss 5.124334
Epoch 2 Batch 210, train loss 5.110933
Epoch 2 Batch 220, train loss 5.109772
Epoch 2 Batch 230, train loss 5.099080
Epoch 2 Batch 240, train loss 5.066368
Epoch 2 Batch 250, train loss 5.048684
Epoch 2 Batch 260, train loss 5.034691
Epoch 2 Batch 270, train loss 5.014590
Epoch 2 Batch 280, train loss 4.992209
Epoch 2 Batch 290, train loss 4.996286
Epoch 2 Batch 300, train loss 4.984017
Epoch 2 Batch 310, train loss 4.970255
Epoch 2 Batch 320, train loss 4.951817
Epoch 2 Batch 330, train loss 4.940063
Epoch 2 Batch 340, train loss 4.947716
Epoch 2 Batch 350, train loss 4.946839
Epoch 2 Batch 360, train loss 4.942019
Epoch 2 Batch 370, train loss 4.939594
Epoch 2 Batch 380, train loss 4.958259
Epoch 2 Batch 390, train loss 4.951008
Epoch 2 Batch 400, train loss 4.940020
Epoch 2 Batch 410, train loss 4.941195
Epoch 2 Batch 420, train loss 4.942302
Epoch 2 Batch 430, train loss 4.945339
Epoch 2 Batch 440, train loss 4.950540
Epoch 2 Batch 450, train loss 4.948697
Epoch 2 Batch 460, train loss 4.936369
Epoch 2 Batch 470, train loss 4.936407
Epoch 2 Batch 480, train loss 4.934035
Epoch 2 Batch 490, train loss 4.934793
Epoch 2 Batch 500, train loss 4.943849
Epoch 2 Batch 510, train loss 4.948325
Epoch 2 Batch 520, train loss 4.944872
Epoch 2 Batch 530, train loss 4.927289
Epoch 2 Batch 540, train loss 4.915022
Epoch 2 Batch 550, train loss 4.919851
Epoch 2 Batch 560, train loss 4.929931
Epoch 2 Batch 570, train loss 4.933728
Epoch 2 Batch 580, train loss 4.940147
Epoch 2 Batch 590, train loss 4.930688
Epoch 2 Batch 600, train loss 4.905390
Epoch 2 Batch 610, train loss 4.865038
Epoch 2 Batch 620, train loss 4.834215
Epoch 2 Batch 630, train loss 4.825831
Epoch 2 Batch 640, train loss 4.790106
Epoch 2 Batch 650, train loss 4.757085
Epoch 2 Batch 660, train loss 4.719640
Epoch 2 Batch 670, train loss 4.691051
Epoch 2 Batch 680, train loss 4.654171
Epoch 2 Batch 690, train loss 4.618539
Epoch 2 Batch 700, train loss 4.597596
Epoch 2 Batch 710, train loss 4.586603
Epoch 2 Batch 720, train loss 4.594154
Epoch 2 Batch 730, train loss 4.590927
Epoch 2 Batch 740, train loss 4.600465
Epoch 2 Batch 750, train loss 4.594111
Epoch 2 Batch 760, train loss 4.586261
Epoch 2 Batch 770, train loss 4.574562
Epoch 2 Batch 780, train loss 4.570295
Epoch 2 Batch 790, train loss 4.564764
Epoch 2 Batch 800, train loss 4.563491
Epoch 2 Batch 810, train loss 4.560769
Epoch 2 Batch 820, train loss 4.541527
Epoch 2 Batch 830, train loss 4.523249
Epoch 2 Batch 840, train loss 4.512219
Epoch 2 Batch 850, train loss 4.496542
Epoch 2 Batch 860, train loss 4.496760
Epoch 2 Batch 870, train loss 4.491871
Epoch 2 Batch 880, train loss 4.478239
Epoch 2 Batch 890, train loss 4.478867
Epoch 2 Batch 900, train loss 4.475482
Epoch 2 Batch 910, train loss 4.467523
Epoch 2 Batch 920, train loss 4.458270
Epoch 2 Batch 930, train loss 4.461526
Validation loss: 183.437, becomes larger. Stop training.
