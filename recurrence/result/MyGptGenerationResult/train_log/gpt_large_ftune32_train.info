[38;5;2m[i 0109 11:00:53.988372 60 compiler.py:955] Jittor(1.3.6.10) src: /home/chenzz/.conda/envs/HW3/lib/python3.7/site-packages/jittor[m
[38;5;2m[i 0109 11:00:53.991546 60 compiler.py:956] g++ at /usr/bin/g++(7.5.0)[m
[38;5;2m[i 0109 11:00:53.991667 60 compiler.py:957] cache_path: /home/chenzz/.cache/jittor/jt1.3.6/g++7.5.0/py3.7.6/Linux-5.3.18-5x95/AMDRyzen73700Xx25/default[m
[38;5;2m[i 0109 11:00:54.809436 60 install_cuda.py:93] cuda_driver_version: [11, 3][m
[38;5;2m[i 0109 11:00:54.809746 60 install_cuda.py:81] restart /home/chenzz/.conda/envs/HW3/bin/python ['main.py', '--name', 'gpt-large-ftune32', '--num_epochs', '30', '--model_config', 'config_large.json', '--pretrain_dir', 'pretrain/gpt2-large-paras.jt', '--batch_size', '16'][m
[38;5;2m[i 0109 11:00:54.954010 44 compiler.py:955] Jittor(1.3.6.10) src: /home/chenzz/.conda/envs/HW3/lib/python3.7/site-packages/jittor[m
[38;5;2m[i 0109 11:00:54.957141 44 compiler.py:956] g++ at /usr/bin/g++(7.5.0)[m
[38;5;2m[i 0109 11:00:54.957256 44 compiler.py:957] cache_path: /home/chenzz/.cache/jittor/jt1.3.6/g++7.5.0/py3.7.6/Linux-5.3.18-5x95/AMDRyzen73700Xx25/default[m
[38;5;2m[i 0109 11:00:55.793653 44 install_cuda.py:93] cuda_driver_version: [11, 3][m
[38;5;2m[i 0109 11:00:55.798025 44 __init__.py:411] Found /home/chenzz/.cache/jittor/jtcuda/cuda11.2_cudnn8_linux/bin/nvcc(11.2.152) at /home/chenzz/.cache/jittor/jtcuda/cuda11.2_cudnn8_linux/bin/nvcc.[m
[38;5;2m[i 0109 11:00:55.800946 44 __init__.py:411] Found addr2line(2.35.1) at /usr/bin/addr2line.[m
[38;5;2m[i 0109 11:00:56.792155 44 compiler.py:1010] cuda key:cu11.2.152_sm_86[m
[38;5;2m[i 0109 11:00:56.954889 44 __init__.py:227] Total mem: 125.72GB, using 16 procs for compiling.[m
[38;5;2m[i 0109 11:00:57.036081 44 jit_compiler.cc:28] Load cc_path: /usr/bin/g++[m
[38;5;2m[i 0109 11:00:57.842772 44 init.cc:62] Found cuda archs: [86,][m
[38;5;2m[i 0109 11:00:57.863393 44 compile_extern.py:522] mpicc not found, distribution disabled.[m
[38;5;2m[i 0109 11:00:58.835663 44 cuda_flags.cc:39] CUDA enabled.[m
Namespace(batch_size=16, cpu_count=1, data_dir='./data', decode_strategy='random', learning_rate=0.0001, maxlen=35, model_config='config_large.json', name='gpt-large-ftune32', num_epochs=30, prefix=0, pretrain_dir='pretrain/gpt2-large-paras.jt', temperature=1, test=None, tokenizer_dir='./tokenizer', top_k=40, top_p=1.0, train_dir='./train_test')
Tokenizer PAD ID: 50256
Loading Data ...
509
[50257,1280,]
[1024,1280,]
[1280,]
[1280,]
[1,1,1024,1024,]
[1,]
[1280,3840,]
[3840,]
[1280,1280,]
[1280,]
[1280,]
[1280,]
[1280,5120,]
[5120,]
[5120,1280,]
[1280,]
[1280,]
[1280,]
[1,1,1024,1024,]
[1,]
[1280,3840,]
[3840,]
[1280,1280,]
[1280,]
[1280,]
[1280,]
[1280,5120,]
[5120,]
[5120,1280,]
[1280,]
[1280,]
[1280,]
[1,1,1024,1024,]
[1,]
[1280,3840,]
[3840,]
[1280,1280,]
[1280,]
[1280,]
[1280,]
[1280,5120,]
[5120,]
[5120,1280,]
[1280,]
[1280,]
[1280,]
[1,1,1024,1024,]
[1,]
[1280,3840,]
[3840,]
[1280,1280,]
[1280,]
[1280,]
[1280,]
[1280,5120,]
[5120,]
[5120,1280,]
[1280,]
[1280,]
[1280,]
[1,1,1024,1024,]
[1,]
[1280,3840,]
[3840,]
[1280,1280,]
[1280,]
[1280,]
[1280,]
[1280,5120,]
[5120,]
[5120,1280,]
[1280,]
[1280,]
[1280,]
[1,1,1024,1024,]
[1,]
[1280,3840,]
[3840,]
[1280,1280,]
[1280,]
[1280,]
[1280,]
[1280,5120,]
[5120,]
[5120,1280,]
[1280,]
[1280,]
[1280,]
[1,1,1024,1024,]
[1,]
[1280,3840,]
[3840,]
[1280,1280,]
[1280,]
[1280,]
[1280,]
[1280,5120,]
[5120,]
[5120,1280,]
[1280,]
[1280,]
[1280,]
[1,1,1024,1024,]
[1,]
[1280,3840,]
[3840,]
[1280,1280,]
[1280,]
[1280,]
[1280,]
[1280,5120,]
[5120,]
[5120,1280,]
[1280,]
[1280,]
[1280,]
[1,1,1024,1024,]
[1,]
[1280,3840,]
[3840,]
[1280,1280,]
[1280,]
[1280,]
[1280,]
[1280,5120,]
[5120,]
[5120,1280,]
[1280,]
[1280,]
[1280,]
[1,1,1024,1024,]
[1,]
[1280,3840,]
[3840,]
[1280,1280,]
[1280,]
[1280,]
[1280,]
[1280,5120,]
[5120,]
[5120,1280,]
[1280,]
[1280,]
[1280,]
[1,1,1024,1024,]
[1,]
[1280,3840,]
[3840,]
[1280,1280,]
[1280,]
[1280,]
[1280,]
[1280,5120,]
[5120,]
[5120,1280,]
[1280,]
[1280,]
[1280,]
[1,1,1024,1024,]
[1,]
[1280,3840,]
[3840,]
[1280,1280,]
[1280,]
[1280,]
[1280,]
[1280,5120,]
[5120,]
[5120,1280,]
[1280,]
[1280,]
[1280,]
[1,1,1024,1024,]
[1,]
[1280,3840,]
[3840,]
[1280,1280,]
[1280,]
[1280,]
[1280,]
[1280,5120,]
[5120,]
[5120,1280,]
[1280,]
[1280,]
[1280,]
[1,1,1024,1024,]
[1,]
[1280,3840,]
[3840,]
[1280,1280,]
[1280,]
[1280,]
[1280,]
[1280,5120,]
[5120,]
[5120,1280,]
[1280,]
[1280,]
[1280,]
[1,1,1024,1024,]
[1,]
[1280,3840,]
[3840,]
[1280,1280,]
[1280,]
[1280,]
[1280,]
[1280,5120,]
[5120,]
[5120,1280,]
[1280,]
[1280,]
[1280,]
[1,1,1024,1024,]
[1,]
[1280,3840,]
[3840,]
[1280,1280,]
[1280,]
[1280,]
[1280,]
[1280,5120,]
[5120,]
[5120,1280,]
[1280,]
[1280,]
[1280,]
[1,1,1024,1024,]
[1,]
[1280,3840,]
[3840,]
[1280,1280,]
[1280,]
[1280,]
[1280,]
[1280,5120,]
[5120,]
[5120,1280,]
[1280,]
[1280,]
[1280,]
[1,1,1024,1024,]
[1,]
[1280,3840,]
[3840,]
[1280,1280,]
[1280,]
[1280,]
[1280,]
[1280,5120,]
[5120,]
[5120,1280,]
[1280,]
[1280,]
[1280,]
[1,1,1024,1024,]
[1,]
[1280,3840,]
[3840,]
[1280,1280,]
[1280,]
[1280,]
[1280,]
[1280,5120,]
[5120,]
[5120,1280,]
[1280,]
[1280,]
[1280,]
[1,1,1024,1024,]
[1,]
[1280,3840,]
[3840,]
[1280,1280,]
[1280,]
[1280,]
[1280,]
[1280,5120,]
[5120,]
[5120,1280,]
[1280,]
[1280,]
[1280,]
[1,1,1024,1024,]
[1,]
[1280,3840,]
[3840,]
[1280,1280,]
[1280,]
[1280,]
[1280,]
[1280,5120,]
[5120,]
[5120,1280,]
[1280,]
[1280,]
[1280,]
[1,1,1024,1024,]
[1,]
[1280,3840,]
[3840,]
[1280,1280,]
[1280,]
[1280,]
[1280,]
[1280,5120,]
[5120,]
[5120,1280,]
[1280,]
[1280,]
[1280,]
[1,1,1024,1024,]
[1,]
[1280,3840,]
[3840,]
[1280,1280,]
[1280,]
[1280,]
[1280,]
[1280,5120,]
[5120,]
[5120,1280,]
[1280,]
[1280,]
[1280,]
[1,1,1024,1024,]
[1,]
[1280,3840,]
[3840,]
[1280,1280,]
[1280,]
[1280,]
[1280,]
[1280,5120,]
[5120,]
[5120,1280,]
[1280,]
[1280,]
[1280,]
[1,1,1024,1024,]
[1,]
[1280,3840,]
[3840,]
[1280,1280,]
[1280,]
[1280,]
[1280,]
[1280,5120,]
[5120,]
[5120,1280,]
[1280,]
[1280,]
[1280,]
[1,1,1024,1024,]
[1,]
[1280,3840,]
[3840,]
[1280,1280,]
[1280,]
[1280,]
[1280,]
[1280,5120,]
[5120,]
[5120,1280,]
[1280,]
[1280,]
[1280,]
[1,1,1024,1024,]
[1,]
[1280,3840,]
[3840,]
[1280,1280,]
[1280,]
[1280,]
[1280,]
[1280,5120,]
[5120,]
[5120,1280,]
[1280,]
[1280,]
[1280,]
[1,1,1024,1024,]
[1,]
[1280,3840,]
[3840,]
[1280,1280,]
[1280,]
[1280,]
[1280,]
[1280,5120,]
[5120,]
[5120,1280,]
[1280,]
[1280,]
[1280,]
[1,1,1024,1024,]
[1,]
[1280,3840,]
[3840,]
[1280,1280,]
[1280,]
[1280,]
[1280,]
[1280,5120,]
[5120,]
[5120,1280,]
[1280,]
[1280,]
[1280,]
[1,1,1024,1024,]
[1,]
[1280,3840,]
[3840,]
[1280,1280,]
[1280,]
[1280,]
[1280,]
[1280,5120,]
[5120,]
[5120,1280,]
[1280,]
[1280,]
[1280,]
[1,1,1024,1024,]
[1,]
[1280,3840,]
[3840,]
[1280,1280,]
[1280,]
[1280,]
[1280,]
[1280,5120,]
[5120,]
[5120,1280,]
[1280,]
[1280,]
[1280,]
[1,1,1024,1024,]
[1,]
[1280,3840,]
[3840,]
[1280,1280,]
[1280,]
[1280,]
[1280,]
[1280,5120,]
[5120,]
[5120,1280,]
[1280,]
[1280,]
[1280,]
[1,1,1024,1024,]
[1,]
[1280,3840,]
[3840,]
[1280,1280,]
[1280,]
[1280,]
[1280,]
[1280,5120,]
[5120,]
[5120,1280,]
[1280,]
[1280,]
[1280,]
[1,1,1024,1024,]
[1,]
[1280,3840,]
[3840,]
[1280,1280,]
[1280,]
[1280,]
[1280,]
[1280,5120,]
[5120,]
[5120,1280,]
[1280,]
[1280,]
[1280,]
[1,1,1024,1024,]
[1,]
[1280,3840,]
[3840,]
[1280,1280,]
[1280,]
[1280,]
[1280,]
[1280,5120,]
[5120,]
[5120,1280,]
[1280,]
[1280,]
[1280,]
[1,1,1024,1024,]
[1,]
[1280,3840,]
[3840,]
[1280,1280,]
[1280,]
[1280,]
[1280,]
[1280,5120,]
[5120,]
[5120,1280,]
[1280,]
[1280,]
[1280,]
[50257,1280,]
jittor_model info:
TfmrLMHeadModel(
    transformer: TfmrModel(
        wte: Embedding(50257, 1280)
        wpe: Embedding(1024, 1280)
        drop: Dropout(0.1, is_train=False)
        h: Sequential(
            0: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            1: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            2: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            3: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            4: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            5: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            6: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            7: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            8: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            9: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            10: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            11: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            12: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            13: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            14: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            15: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            16: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            17: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            18: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            19: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            20: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            21: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            22: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            23: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            24: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            25: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            26: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            27: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            28: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            29: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            30: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            31: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            32: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            33: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            34: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            35: TfmrBlock(
                ln_1: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(3840, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((1280,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(5120, None, None)
                    c_proj: TransposeLinear(1280, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
        )
        ln_f: LayerNorm((1280,), 1e-05, elementwise_affine=True)
    )
    lm_head: Linear(1280, 50257, None, None)
)[38;5;3m[w 0109 11:01:04.334249 44 grad.cc:77] grads[4] 'transformer.h.0.attn.masked_bias' doesn't have gradient. It will be set to zero: Var(9029:1:1:1:i0:o0:s1:n0,float32,transformer.h.0.attn.masked_bias,7f7033600000)[1,][m
[38;5;3m[w 0109 11:01:04.334276 44 grad.cc:77] grads[17] 'transformer.h.1.attn.masked_bias' doesn't have gradient. It will be set to zero: Var(9057:1:1:1:i0:o0:s1:n0,float32,transformer.h.1.attn.masked_bias,7f7033600200)[1,][m
[38;5;3m[w 0109 11:01:04.334283 44 grad.cc:77] grads[30] 'transformer.h.2.attn.masked_bias' doesn't have gradient. It will be set to zero: Var(9085:1:1:1:i0:o0:s1:n0,float32,transformer.h.2.attn.masked_bias,7f7033600400)[1,][m
[38;5;3m[w 0109 11:01:04.334288 44 grad.cc:77] grads[43] 'transformer.h.3.attn.masked_bias' doesn't have gradient. It will be set to zero: Var(9113:1:1:1:i0:o0:s1:n0,float32,transformer.h.3.attn.masked_bias,7f7033600600)[1,][m
[38;5;3m[w 0109 11:01:04.334294 44 grad.cc:77] grads[56] 'transformer.h.4.attn.masked_bias' doesn't have gradient. It will be set to zero: Var(9141:1:1:1:i0:o0:s1:n0,float32,transformer.h.4.attn.masked_bias,7f7033600800)[1,][m
[38;5;3m[w 0109 11:01:04.334300 44 grad.cc:77] grads[69] 'transformer.h.5.attn.masked_bias' doesn't have gradient. It will be set to zero: Var(9169:1:1:1:i0:o0:s1:n0,float32,transformer.h.5.attn.masked_bias,7f7033600a00)[1,][m
[38;5;3m[w 0109 11:01:04.334308 44 grad.cc:77] grads[82] 'transformer.h.6.attn.masked_bias' doesn't have gradient. It will be set to zero: Var(9197:1:1:1:i0:o0:s1:n0,float32,transformer.h.6.attn.masked_bias,7f7033600c00)[1,][m
[38;5;3m[w 0109 11:01:04.334318 44 grad.cc:77] grads[95] 'transformer.h.7.attn.masked_bias' doesn't have gradient. It will be set to zero: Var(9225:1:1:1:i0:o0:s1:n0,float32,transformer.h.7.attn.masked_bias,7f7033600e00)[1,][m
[38;5;3m[w 0109 11:01:04.334325 44 grad.cc:77] grads[108] 'transformer.h.8.attn.masked_bias' doesn't have gradient. It will be set to zero: Var(9253:1:1:1:i0:o0:s1:n0,float32,transformer.h.8.attn.masked_bias,7f7033601000)[1,][m
[38;5;3m[w 0109 11:01:04.334333 44 grad.cc:77] grads[121] 'transformer.h.9.attn.masked_bias' doesn't have gradient. It will be set to zero: Var(9281:1:1:1:i0:o0:s1:n0,float32,transformer.h.9.attn.masked_bias,7f7033601200)[1,][m
[38;5;3m[w 0109 11:01:04.334342 44 grad.cc:77] grads[134] 'transformer.h.10.attn.masked_bias' doesn't have gradient. It will be set to zero: Var(9309:1:1:1:i0:o0:s1:n0,float32,transformer.h.10.attn.masked_bias,7f7033601400)[1,][m
[38;5;3m[w 0109 11:01:04.334352 44 grad.cc:77] grads[147] 'transformer.h.11.attn.masked_bias' doesn't have gradient. It will be set to zero: Var(9337:1:1:1:i0:o0:s1:n0,float32,transformer.h.11.attn.masked_bias,7f7033601600)[1,][m
[38;5;3m[w 0109 11:01:04.334362 44 grad.cc:77] grads[160] 'transformer.h.12.attn.masked_bias' doesn't have gradient. It will be set to zero: Var(9365:1:1:1:i0:o0:s1:n0,float32,transformer.h.12.attn.masked_bias,7f7033601800)[1,][m
[38;5;3m[w 0109 11:01:04.334370 44 grad.cc:77] grads[173] 'transformer.h.13.attn.masked_bias' doesn't have gradient. It will be set to zero: Var(9393:1:1:1:i0:o0:s1:n0,float32,transformer.h.13.attn.masked_bias,7f7033601a00)[1,][m
[38;5;3m[w 0109 11:01:04.334384 44 grad.cc:77] grads[186] 'transformer.h.14.attn.masked_bias' doesn't have gradient. It will be set to zero: Var(9421:1:1:1:i0:o0:s1:n0,float32,transformer.h.14.attn.masked_bias,7f7033601c00)[1,][m
[38;5;3m[w 0109 11:01:04.334391 44 grad.cc:77] grads[199] 'transformer.h.15.attn.masked_bias' doesn't have gradient. It will be set to zero: Var(9449:1:1:1:i0:o0:s1:n0,float32,transformer.h.15.attn.masked_bias,7f7033601e00)[1,][m
[38;5;3m[w 0109 11:01:04.334399 44 grad.cc:77] grads[212] 'transformer.h.16.attn.masked_bias' doesn't have gradient. It will be set to zero: Var(9477:1:1:1:i0:o0:s1:n0,float32,transformer.h.16.attn.masked_bias,7f7033602000)[1,][m
[38;5;3m[w 0109 11:01:04.334409 44 grad.cc:77] grads[225] 'transformer.h.17.attn.masked_bias' doesn't have gradient. It will be set to zero: Var(9505:1:1:1:i0:o0:s1:n0,float32,transformer.h.17.attn.masked_bias,7f7033602200)[1,][m
[38;5;3m[w 0109 11:01:04.334418 44 grad.cc:77] grads[238] 'transformer.h.18.attn.masked_bias' doesn't have gradient. It will be set to zero: Var(9533:1:1:1:i0:o0:s1:n0,float32,transformer.h.18.attn.masked_bias,7f7033602400)[1,][m
[38;5;3m[w 0109 11:01:04.334427 44 grad.cc:77] grads[251] 'transformer.h.19.attn.masked_bias' doesn't have gradient. It will be set to zero: Var(9561:1:1:1:i0:o0:s1:n0,float32,transformer.h.19.attn.masked_bias,7f7033602600)[1,][m
[38;5;3m[w 0109 11:01:04.334435 44 grad.cc:77] grads[264] 'transformer.h.20.attn.masked_bias' doesn't have gradient. It will be set to zero: Var(9589:1:1:1:i0:o0:s1:n0,float32,transformer.h.20.attn.masked_bias,7f7033602800)[1,][m
[38;5;3m[w 0109 11:01:04.334445 44 grad.cc:77] grads[277] 'transformer.h.21.attn.masked_bias' doesn't have gradient. It will be set to zero: Var(9617:1:1:1:i0:o0:s1:n0,float32,transformer.h.21.attn.masked_bias,7f7033602a00)[1,][m
[38;5;3m[w 0109 11:01:04.334453 44 grad.cc:77] grads[290] 'transformer.h.22.attn.masked_bias' doesn't have gradient. It will be set to zero: Var(9645:1:1:1:i0:o0:s1:n0,float32,transformer.h.22.attn.masked_bias,7f7033602c00)[1,][m
[38;5;3m[w 0109 11:01:04.334464 44 grad.cc:77] grads[303] 'transformer.h.23.attn.masked_bias' doesn't have gradient. It will be set to zero: Var(9673:1:1:1:i0:o0:s1:n0,float32,transformer.h.23.attn.masked_bias,7f7033602e00)[1,][m
[38;5;3m[w 0109 11:01:04.334474 44 grad.cc:77] grads[316] 'transformer.h.24.attn.masked_bias' doesn't have gradient. It will be set to zero: Var(9701:1:1:1:i0:o0:s1:n0,float32,transformer.h.24.attn.masked_bias,7f7033603000)[1,][m
[38;5;3m[w 0109 11:01:04.334484 44 grad.cc:77] grads[329] 'transformer.h.25.attn.masked_bias' doesn't have gradient. It will be set to zero: Var(9729:1:1:1:i0:o0:s1:n0,float32,transformer.h.25.attn.masked_bias,7f7033603200)[1,][m
[38;5;3m[w 0109 11:01:04.334494 44 grad.cc:77] grads[342] 'transformer.h.26.attn.masked_bias' doesn't have gradient. It will be set to zero: Var(9757:1:1:1:i0:o0:s1:n0,float32,transformer.h.26.attn.masked_bias,7f7033603400)[1,][m
[38;5;3m[w 0109 11:01:04.334503 44 grad.cc:77] grads[355] 'transformer.h.27.attn.masked_bias' doesn't have gradient. It will be set to zero: Var(9785:1:1:1:i0:o0:s1:n0,float32,transformer.h.27.attn.masked_bias,7f7033603600)[1,][m
[38;5;3m[w 0109 11:01:04.334510 44 grad.cc:77] grads[368] 'transformer.h.28.attn.masked_bias' doesn't have gradient. It will be set to zero: Var(9813:1:1:1:i0:o0:s1:n0,float32,transformer.h.28.attn.masked_bias,7f7033603800)[1,][m
[38;5;3m[w 0109 11:01:04.334522 44 grad.cc:77] grads[381] 'transformer.h.29.attn.masked_bias' doesn't have gradient. It will be set to zero: Var(9841:1:1:1:i0:o0:s1:n0,float32,transformer.h.29.attn.masked_bias,7f7033603a00)[1,][m
[38;5;3m[w 0109 11:01:04.334530 44 grad.cc:77] grads[394] 'transformer.h.30.attn.masked_bias' doesn't have gradient. It will be set to zero: Var(9869:1:1:1:i0:o0:s1:n0,float32,transformer.h.30.attn.masked_bias,7f7033603c00)[1,][m
[38;5;3m[w 0109 11:01:04.334539 44 grad.cc:77] grads[407] 'transformer.h.31.attn.masked_bias' doesn't have gradient. It will be set to zero: Var(9897:1:1:1:i0:o0:s1:n0,float32,transformer.h.31.attn.masked_bias,7f7033603e00)[1,][m
[38;5;3m[w 0109 11:01:04.334552 44 grad.cc:77] grads[420] 'transformer.h.32.attn.masked_bias' doesn't have gradient. It will be set to zero: Var(9925:1:1:1:i0:o0:s1:n0,float32,transformer.h.32.attn.masked_bias,7f7033604000)[1,][m
[38;5;3m[w 0109 11:01:04.334560 44 grad.cc:77] grads[433] 'transformer.h.33.attn.masked_bias' doesn't have gradient. It will be set to zero: Var(9953:1:1:1:i0:o0:s1:n0,float32,transformer.h.33.attn.masked_bias,7f7033604200)[1,][m
[38;5;3m[w 0109 11:01:04.334570 44 grad.cc:77] grads[446] 'transformer.h.34.attn.masked_bias' doesn't have gradient. It will be set to zero: Var(9981:1:1:1:i0:o0:s1:n0,float32,transformer.h.34.attn.masked_bias,7f7033604400)[1,][m
[38;5;3m[w 0109 11:01:04.334580 44 grad.cc:77] grads[459] 'transformer.h.35.attn.masked_bias' doesn't have gradient. It will be set to zero: Var(10009:1:1:1:i0:o0:s1:n0,float32,transformer.h.35.attn.masked_bias,7f7033604600)[1,][m

Compiling Operators(39/39) used: 2.32s eta:    0s 

Epoch 1 Batch 10, train loss 8.575214
Epoch 1 Batch 20, train loss 7.434661
Epoch 1 Batch 30, train loss 6.826516
Epoch 1 Batch 40, train loss 6.479504
Epoch 1 Batch 50, train loss 6.267698
Epoch 1 Batch 60, train loss 6.090830
Epoch 1 Batch 70, train loss 5.926308
Epoch 1 Batch 80, train loss 5.768327
Epoch 1 Batch 90, train loss 5.613440
Epoch 1 Batch 100, train loss 5.488533
Epoch 1 Batch 110, train loss 5.057561
Epoch 1 Batch 120, train loss 4.855482
Epoch 1 Batch 130, train loss 4.703846
Epoch 1 Batch 140, train loss 4.559841
Epoch 1 Batch 150, train loss 4.397577
Epoch 1 Batch 160, train loss 4.270659
Epoch 1 Batch 170, train loss 4.231992
Epoch 1 Batch 180, train loss 4.193181
Epoch 1 Batch 190, train loss 4.171427
Epoch 1 Batch 200, train loss 4.145957
Epoch 1 Batch 210, train loss 4.119375
Epoch 1 Batch 220, train loss 4.080379
Epoch 1 Batch 230, train loss 4.050243
Epoch 1 Batch 240, train loss 4.011670
Epoch 1 Batch 250, train loss 3.978291
Epoch 1 Batch 260, train loss 3.937985
Epoch 1 Batch 270, train loss 3.819495
Epoch 1 Batch 280, train loss 3.708593
Epoch 1 Batch 290, train loss 3.638492
Epoch 1 Batch 300, train loss 3.558394
Epoch 1 Batch 310, train loss 3.478072
Epoch 1 Batch 320, train loss 3.412282
Epoch 1 Batch 330, train loss 3.345546
Epoch 1 Batch 340, train loss 3.314041
Epoch 1 Batch 350, train loss 3.275483
Epoch 1 Batch 360, train loss 3.218095
Epoch 1 Batch 370, train loss 3.182095
Epoch 1 Batch 380, train loss 3.178088
Epoch 1 Batch 390, train loss 3.140533
Epoch 1 Batch 400, train loss 3.104378
Epoch 1 Batch 410, train loss 3.085273
Epoch 1 Batch 420, train loss 3.058412
Epoch 1 Batch 430, train loss 3.043069
Epoch 1 Batch 440, train loss 3.007904
Epoch 1 Batch 450, train loss 2.982811
Epoch 1 Batch 460, train loss 2.946170
Epoch 1 Batch 470, train loss 2.910606
Epoch 1 Batch 480, train loss 2.863822
Epoch 1 Batch 490, train loss 2.827438
Epoch 1 Batch 500, train loss 2.806863
Epoch 1 Batch 510, train loss 2.784093
Epoch 1 Batch 520, train loss 2.754167
Epoch 1 Batch 530, train loss 2.704290
Epoch 1 Batch 540, train loss 2.661274
Epoch 1 Batch 550, train loss 2.645852
Epoch 1 Batch 560, train loss 2.649412
Epoch 1 Batch 570, train loss 2.638899
Epoch 1 Batch 580, train loss 2.651951
Epoch 1 Batch 590, train loss 2.633995
Epoch 1 Batch 600, train loss 2.642320
Epoch 1 Batch 610, train loss 2.626897
Epoch 1 Batch 620, train loss 2.614361
Epoch 1 Batch 630, train loss 2.634997
Epoch 1 Batch 640, train loss 2.626866
Epoch 1 Batch 650, train loss 2.599588
Epoch 1 Batch 660, train loss 2.567684
Epoch 1 Batch 670, train loss 2.557708
Epoch 1 Batch 680, train loss 2.521853
Epoch 1 Batch 690, train loss 2.492384
Epoch 1 Batch 700, train loss 2.428891
Epoch 1 Batch 710, train loss 2.367970
Epoch 1 Batch 720, train loss 2.367419
Epoch 1 Batch 730, train loss 2.334074
Epoch 1 Batch 740, train loss 2.319389
Epoch 1 Batch 750, train loss 2.309768
Epoch 1 Batch 760, train loss 2.281218
Epoch 1 Batch 770, train loss 2.227924
Epoch 1 Batch 780, train loss 2.211430
Epoch 1 Batch 790, train loss 2.188895
Epoch 1 Batch 800, train loss 2.168027
Epoch 1 Batch 810, train loss 2.154513
Epoch 1 Batch 820, train loss 2.098329
Epoch 1 Batch 830, train loss 2.059183
Epoch 1 Batch 840, train loss 2.030974
Epoch 1 Batch 850, train loss 1.990970
Epoch 1 Batch 860, train loss 1.978565
Epoch 1 Batch 870, train loss 1.971765
Epoch 1 Batch 880, train loss 1.935015
Epoch 1 Batch 890, train loss 1.916847
Epoch 1 Batch 900, train loss 1.910129
Epoch 1 Batch 910, train loss 1.891665
Epoch 1 Batch 920, train loss 1.880164
Epoch 1 Batch 930, train loss 1.877927
Epoch 1 of 30 took 361.9329581260681s
  training loss:                 3.0879280043563355
  validation loss:               2.6638554492235182
  validation perplexity:         14.351514105579673
  best epoch:                    1
  best validation perplexity:    14.351514105579673
Epoch 2 Batch 10, train loss 2.413877
Epoch 2 Batch 20, train loss 2.443789
Epoch 2 Batch 30, train loss 2.422541
Epoch 2 Batch 40, train loss 2.446658
Epoch 2 Batch 50, train loss 2.510185
Epoch 2 Batch 60, train loss 2.532431
Epoch 2 Batch 70, train loss 2.547481
Epoch 2 Batch 80, train loss 2.548032
Epoch 2 Batch 90, train loss 2.543401
Epoch 2 Batch 100, train loss 2.533787
Epoch 2 Batch 110, train loss 2.550218
Epoch 2 Batch 120, train loss 2.560564
Epoch 2 Batch 130, train loss 2.570917
Epoch 2 Batch 140, train loss 2.573184
Epoch 2 Batch 150, train loss 2.551736
Epoch 2 Batch 160, train loss 2.535589
Epoch 2 Batch 170, train loss 2.507816
Epoch 2 Batch 180, train loss 2.481621
Epoch 2 Batch 190, train loss 2.451322
Epoch 2 Batch 200, train loss 2.448620
Epoch 2 Batch 210, train loss 2.421036
Epoch 2 Batch 220, train loss 2.388188
Epoch 2 Batch 230, train loss 2.353909
Epoch 2 Batch 240, train loss 2.303300
Epoch 2 Batch 250, train loss 2.258175
Epoch 2 Batch 260, train loss 2.228100
Epoch 2 Batch 270, train loss 2.196005
Epoch 2 Batch 280, train loss 2.155009
Epoch 2 Batch 290, train loss 2.155998
Epoch 2 Batch 300, train loss 2.128080
Epoch 2 Batch 310, train loss 2.105017
Epoch 2 Batch 320, train loss 2.090605
Epoch 2 Batch 330, train loss 2.082110
Epoch 2 Batch 340, train loss 2.096358
Epoch 2 Batch 350, train loss 2.089776
Epoch 2 Batch 360, train loss 2.074759
Epoch 2 Batch 370, train loss 2.077083
Epoch 2 Batch 380, train loss 2.095703
Epoch 2 Batch 390, train loss 2.089483
Epoch 2 Batch 400, train loss 2.069214
Epoch 2 Batch 410, train loss 2.065576
Epoch 2 Batch 420, train loss 2.060638
Epoch 2 Batch 430, train loss 2.060940
Epoch 2 Batch 440, train loss 2.047111
Epoch 2 Batch 450, train loss 2.038189
Epoch 2 Batch 460, train loss 2.023093
Epoch 2 Batch 470, train loss 2.010019
Epoch 2 Batch 480, train loss 1.996576
Epoch 2 Batch 490, train loss 1.984496
Epoch 2 Batch 500, train loss 1.993720
Epoch 2 Batch 510, train loss 1.989217
Epoch 2 Batch 520, train loss 1.977613
Epoch 2 Batch 530, train loss 1.956740
Epoch 2 Batch 540, train loss 1.939242
Epoch 2 Batch 550, train loss 1.946055
Epoch 2 Batch 560, train loss 1.953436
Epoch 2 Batch 570, train loss 1.958433
Epoch 2 Batch 580, train loss 1.972878
Epoch 2 Batch 590, train loss 1.964745
Epoch 2 Batch 600, train loss 1.945217
Epoch 2 Batch 610, train loss 1.919015
Epoch 2 Batch 620, train loss 1.892318
Epoch 2 Batch 630, train loss 1.890470
Epoch 2 Batch 640, train loss 1.874386
Epoch 2 Batch 650, train loss 1.836112
Epoch 2 Batch 660, train loss 1.798767
Epoch 2 Batch 670, train loss 1.769423
Epoch 2 Batch 680, train loss 1.733479
Epoch 2 Batch 690, train loss 1.700872
Epoch 2 Batch 700, train loss 1.669572
Epoch 2 Batch 710, train loss 1.640681
Epoch 2 Batch 720, train loss 1.652544
Epoch 2 Batch 730, train loss 1.640780
Epoch 2 Batch 740, train loss 1.636439
Epoch 2 Batch 750, train loss 1.642429
Epoch 2 Batch 760, train loss 1.639621
Epoch 2 Batch 770, train loss 1.613185
Epoch 2 Batch 780, train loss 1.608554
Epoch 2 Batch 790, train loss 1.590674
Epoch 2 Batch 800, train loss 1.586742
Epoch 2 Batch 810, train loss 1.582165
Epoch 2 Batch 820, train loss 1.546987
Epoch 2 Batch 830, train loss 1.521566
Epoch 2 Batch 840, train loss 1.503928
Epoch 2 Batch 850, train loss 1.473808
Epoch 2 Batch 860, train loss 1.459991
Epoch 2 Batch 870, train loss 1.452521
Epoch 2 Batch 880, train loss 1.427529
Epoch 2 Batch 890, train loss 1.416601
Epoch 2 Batch 900, train loss 1.407463
Epoch 2 Batch 910, train loss 1.386677
Epoch 2 Batch 920, train loss 1.368953
Epoch 2 Batch 930, train loss 1.355018
Validation loss: 14.352, becomes larger. Stop training.
