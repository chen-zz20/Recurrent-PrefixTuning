[38;5;2m[i 0109 11:52:12.830970 36 compiler.py:955] Jittor(1.3.6.10) src: /home/chenzz/.conda/envs/HW3/lib/python3.7/site-packages/jittor[m
[38;5;2m[i 0109 11:52:12.834216 36 compiler.py:956] g++ at /usr/bin/g++(7.5.0)[m
[38;5;2m[i 0109 11:52:12.834344 36 compiler.py:957] cache_path: /home/chenzz/.cache/jittor/jt1.3.6/g++7.5.0/py3.7.6/Linux-5.3.18-5x95/AMDRyzen73700Xx25/default[m
[38;5;2m[i 0109 11:52:13.655336 36 install_cuda.py:93] cuda_driver_version: [11, 3][m
[38;5;2m[i 0109 11:52:13.655681 36 install_cuda.py:81] restart /home/chenzz/.conda/envs/HW3/bin/python ['main.py', '--name', 'gpt-prefix32', '--num_epochs', '30', '--prefix', '32', '--model_config', 'config.json', '--pretrain_dir', 'pretrain/gpt2-paras.jt'][m
[38;5;2m[i 0109 11:52:13.803655 04 compiler.py:955] Jittor(1.3.6.10) src: /home/chenzz/.conda/envs/HW3/lib/python3.7/site-packages/jittor[m
[38;5;2m[i 0109 11:52:13.806867 04 compiler.py:956] g++ at /usr/bin/g++(7.5.0)[m
[38;5;2m[i 0109 11:52:13.806979 04 compiler.py:957] cache_path: /home/chenzz/.cache/jittor/jt1.3.6/g++7.5.0/py3.7.6/Linux-5.3.18-5x95/AMDRyzen73700Xx25/default[m
[38;5;2m[i 0109 11:52:14.616928 04 install_cuda.py:93] cuda_driver_version: [11, 3][m
[38;5;2m[i 0109 11:52:14.621449 04 __init__.py:411] Found /home/chenzz/.cache/jittor/jtcuda/cuda11.2_cudnn8_linux/bin/nvcc(11.2.152) at /home/chenzz/.cache/jittor/jtcuda/cuda11.2_cudnn8_linux/bin/nvcc.[m
[38;5;2m[i 0109 11:52:14.626024 04 __init__.py:411] Found addr2line(2.35.1) at /usr/bin/addr2line.[m
[38;5;2m[i 0109 11:52:15.635574 04 compiler.py:1010] cuda key:cu11.2.152_sm_86[m
[38;5;2m[i 0109 11:52:15.797637 04 __init__.py:227] Total mem: 125.72GB, using 16 procs for compiling.[m
[38;5;2m[i 0109 11:52:15.881335 04 jit_compiler.cc:28] Load cc_path: /usr/bin/g++[m
[38;5;2m[i 0109 11:52:16.667631 04 init.cc:62] Found cuda archs: [86,][m
[38;5;2m[i 0109 11:52:16.687418 04 compile_extern.py:522] mpicc not found, distribution disabled.[m
[38;5;2m[i 0109 11:52:17.678931 04 cuda_flags.cc:39] CUDA enabled.[m
Namespace(batch_size=32, cpu_count=1, data_dir='./data', decode_strategy='random', learning_rate=0.0001, maxlen=35, model_config='config.json', name='gpt-prefix32', num_epochs=30, prefix=32, pretrain_dir='pretrain/gpt2-paras.jt', temperature=1, test=None, tokenizer_dir='./tokenizer', top_k=40, top_p=1.0, train_dir='./train_test')
Tokenizer PAD ID: 50256
Loading Data ...
13
[32,768,]
[32,768,]
[32,768,]
[32,768,]
[32,768,]
[32,768,]
[32,768,]
[32,768,]
[32,768,]
[32,768,]
[32,768,]
[32,768,]
[50257,768,]
jittor_model info:
TfmrLMHeadModel(
    transformer: TfmrModel(
        wte: Embedding(50257, 768)
        wpe: Embedding(1024, 768)
        drop: Dropout(0.1, is_train=False)
        h: Sequential(
            0: TfmrBlock(
                ln_1: LayerNorm((768,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(2304, None, None)
                    c_proj: TransposeLinear(768, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((768,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(3072, None, None)
                    c_proj: TransposeLinear(768, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            1: TfmrBlock(
                ln_1: LayerNorm((768,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(2304, None, None)
                    c_proj: TransposeLinear(768, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((768,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(3072, None, None)
                    c_proj: TransposeLinear(768, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            2: TfmrBlock(
                ln_1: LayerNorm((768,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(2304, None, None)
                    c_proj: TransposeLinear(768, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((768,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(3072, None, None)
                    c_proj: TransposeLinear(768, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            3: TfmrBlock(
                ln_1: LayerNorm((768,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(2304, None, None)
                    c_proj: TransposeLinear(768, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((768,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(3072, None, None)
                    c_proj: TransposeLinear(768, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            4: TfmrBlock(
                ln_1: LayerNorm((768,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(2304, None, None)
                    c_proj: TransposeLinear(768, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((768,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(3072, None, None)
                    c_proj: TransposeLinear(768, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            5: TfmrBlock(
                ln_1: LayerNorm((768,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(2304, None, None)
                    c_proj: TransposeLinear(768, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((768,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(3072, None, None)
                    c_proj: TransposeLinear(768, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            6: TfmrBlock(
                ln_1: LayerNorm((768,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(2304, None, None)
                    c_proj: TransposeLinear(768, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((768,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(3072, None, None)
                    c_proj: TransposeLinear(768, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            7: TfmrBlock(
                ln_1: LayerNorm((768,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(2304, None, None)
                    c_proj: TransposeLinear(768, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((768,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(3072, None, None)
                    c_proj: TransposeLinear(768, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            8: TfmrBlock(
                ln_1: LayerNorm((768,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(2304, None, None)
                    c_proj: TransposeLinear(768, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((768,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(3072, None, None)
                    c_proj: TransposeLinear(768, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            9: TfmrBlock(
                ln_1: LayerNorm((768,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(2304, None, None)
                    c_proj: TransposeLinear(768, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((768,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(3072, None, None)
                    c_proj: TransposeLinear(768, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            10: TfmrBlock(
                ln_1: LayerNorm((768,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(2304, None, None)
                    c_proj: TransposeLinear(768, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((768,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(3072, None, None)
                    c_proj: TransposeLinear(768, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            11: TfmrBlock(
                ln_1: LayerNorm((768,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(2304, None, None)
                    c_proj: TransposeLinear(768, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((768,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(3072, None, None)
                    c_proj: TransposeLinear(768, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
        )
        ln_f: LayerNorm((768,), 1e-05, elementwise_affine=True)
    )
    lm_head: Linear(768, 50257, None, None)
)
Epoch 1 Batch 10, train loss 12.319037
Epoch 1 Batch 20, train loss 11.713335
Epoch 1 Batch 30, train loss 11.258616
Epoch 1 Batch 40, train loss 10.887440
Epoch 1 Batch 50, train loss 10.535304
Epoch 1 Batch 60, train loss 10.183471
Epoch 1 Batch 70, train loss 9.821625
Epoch 1 Batch 80, train loss 9.446061
Epoch 1 Batch 90, train loss 9.131391
Epoch 1 Batch 100, train loss 8.843054
Epoch 1 Batch 110, train loss 8.211311
Epoch 1 Batch 120, train loss 7.677371
Epoch 1 Batch 130, train loss 7.207235
Epoch 1 Batch 140, train loss 6.771597
Epoch 1 Batch 150, train loss 6.401110
Epoch 1 Batch 160, train loss 6.084378
Epoch 1 Batch 170, train loss 5.847369
Epoch 1 Batch 180, train loss 5.678915
Epoch 1 Batch 190, train loss 5.531097
Epoch 1 Batch 200, train loss 5.411361
Epoch 1 Batch 210, train loss 5.311472
Epoch 1 Batch 220, train loss 5.232200
Epoch 1 Batch 230, train loss 5.152340
Epoch 1 Batch 240, train loss 5.094926
Epoch 1 Batch 250, train loss 5.038480
Epoch 1 Batch 260, train loss 5.003634
Epoch 1 Batch 270, train loss 4.945292
Epoch 1 Batch 280, train loss 4.914495
Epoch 1 Batch 290, train loss 4.879850
Epoch 1 Batch 300, train loss 4.860401
Epoch 1 Batch 310, train loss 4.852813
Epoch 1 Batch 320, train loss 4.835260
Epoch 1 Batch 330, train loss 4.825042
Epoch 1 Batch 340, train loss 4.820589
Epoch 1 Batch 350, train loss 4.792418
Epoch 1 Batch 360, train loss 4.761274
Epoch 1 Batch 370, train loss 4.756333
Epoch 1 Batch 380, train loss 4.721280
Epoch 1 Batch 390, train loss 4.694027
Epoch 1 Batch 400, train loss 4.645749
Epoch 1 Batch 410, train loss 4.596568
Epoch 1 Batch 420, train loss 4.578634
Epoch 1 Batch 430, train loss 4.555320
Epoch 1 Batch 440, train loss 4.521860
Epoch 1 Batch 450, train loss 4.506953
Epoch 1 Batch 460, train loss 4.478459
Epoch 1 of 30 took 59.45770597457886s
  training loss:                 5.723844028485101
  validation loss:               5.082843339014054
  validation perplexity:         161.2318415857694
  best epoch:                    1
  best validation perplexity:    161.2318415857694
Epoch 2 Batch 10, train loss 4.828909
Epoch 2 Batch 20, train loss 4.851804
Epoch 2 Batch 30, train loss 4.917959
Epoch 2 Batch 40, train loss 4.947770
Epoch 2 Batch 50, train loss 4.941493
Epoch 2 Batch 60, train loss 4.951288
Epoch 2 Batch 70, train loss 4.960030
Epoch 2 Batch 80, train loss 4.964819
Epoch 2 Batch 90, train loss 4.957452
Epoch 2 Batch 100, train loss 4.950056
Epoch 2 Batch 110, train loss 4.959505
Epoch 2 Batch 120, train loss 4.948843
Epoch 2 Batch 130, train loss 4.925060
Epoch 2 Batch 140, train loss 4.889868
Epoch 2 Batch 150, train loss 4.882250
Epoch 2 Batch 160, train loss 4.859646
Epoch 2 Batch 170, train loss 4.833013
Epoch 2 Batch 180, train loss 4.809280
Epoch 2 Batch 190, train loss 4.793980
Epoch 2 Batch 200, train loss 4.780518
Epoch 2 Batch 210, train loss 4.764250
Epoch 2 Batch 220, train loss 4.766976
Epoch 2 Batch 230, train loss 4.753187
Epoch 2 Batch 240, train loss 4.759852
Epoch 2 Batch 250, train loss 4.754227
Epoch 2 Batch 260, train loss 4.753306
Epoch 2 Batch 270, train loss 4.741134
Epoch 2 Batch 280, train loss 4.743575
Epoch 2 Batch 290, train loss 4.748337
Epoch 2 Batch 300, train loss 4.735966
Epoch 2 Batch 310, train loss 4.699074
Epoch 2 Batch 320, train loss 4.659641
Epoch 2 Batch 330, train loss 4.633816
Epoch 2 Batch 340, train loss 4.603777
Epoch 2 Batch 350, train loss 4.551395
Epoch 2 Batch 360, train loss 4.509345
Epoch 2 Batch 370, train loss 4.491988
Epoch 2 Batch 380, train loss 4.444901
Epoch 2 Batch 390, train loss 4.404105
Epoch 2 Batch 400, train loss 4.366522
Epoch 2 Batch 410, train loss 4.350302
Epoch 2 Batch 420, train loss 4.337553
Epoch 2 Batch 430, train loss 4.318517
Epoch 2 Batch 440, train loss 4.297505
Epoch 2 Batch 450, train loss 4.297482
Epoch 2 Batch 460, train loss 4.283367
Validation loss: 161.232, becomes larger. Stop training.
