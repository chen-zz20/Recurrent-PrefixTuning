[38;5;2m[i 0109 10:52:51.493021 64 compiler.py:955] Jittor(1.3.6.10) src: /home/chenzz/.conda/envs/HW3/lib/python3.7/site-packages/jittor[m
[38;5;2m[i 0109 10:52:51.496217 64 compiler.py:956] g++ at /usr/bin/g++(7.5.0)[m
[38;5;2m[i 0109 10:52:51.496353 64 compiler.py:957] cache_path: /home/chenzz/.cache/jittor/jt1.3.6/g++7.5.0/py3.7.6/Linux-5.3.18-5x95/AMDRyzen73700Xx25/default[m
[38;5;2m[i 0109 10:52:51.504847 64 install_cuda.py:93] cuda_driver_version: [11, 3][m
[38;5;2m[i 0109 10:52:51.505191 64 install_cuda.py:81] restart /home/chenzz/.conda/envs/HW3/bin/python ['main.py', '--name', 'gpt-ftune32', '--num_epochs', '30', '--model_config', 'config.json', '--pretrain_dir', 'pretrain/gpt2-paras.jt'][m
[38;5;2m[i 0109 10:52:51.653151 76 compiler.py:955] Jittor(1.3.6.10) src: /home/chenzz/.conda/envs/HW3/lib/python3.7/site-packages/jittor[m
[38;5;2m[i 0109 10:52:51.655913 76 compiler.py:956] g++ at /usr/bin/g++(7.5.0)[m
[38;5;2m[i 0109 10:52:51.655972 76 compiler.py:957] cache_path: /home/chenzz/.cache/jittor/jt1.3.6/g++7.5.0/py3.7.6/Linux-5.3.18-5x95/AMDRyzen73700Xx25/default[m
[38;5;2m[i 0109 10:52:52.482432 76 install_cuda.py:93] cuda_driver_version: [11, 3][m
[38;5;2m[i 0109 10:52:52.487026 76 __init__.py:411] Found /home/chenzz/.cache/jittor/jtcuda/cuda11.2_cudnn8_linux/bin/nvcc(11.2.152) at /home/chenzz/.cache/jittor/jtcuda/cuda11.2_cudnn8_linux/bin/nvcc.[m
[38;5;2m[i 0109 10:52:52.491355 76 __init__.py:411] Found addr2line(2.35.1) at /usr/bin/addr2line.[m
[38;5;2m[i 0109 10:52:53.509525 76 compiler.py:1010] cuda key:cu11.2.152_sm_86[m
[38;5;2m[i 0109 10:52:53.668946 76 __init__.py:227] Total mem: 125.72GB, using 16 procs for compiling.[m
[38;5;2m[i 0109 10:52:53.747910 76 jit_compiler.cc:28] Load cc_path: /usr/bin/g++[m
[38;5;2m[i 0109 10:52:54.586166 76 init.cc:62] Found cuda archs: [86,][m
[38;5;2m[i 0109 10:52:54.606731 76 compile_extern.py:522] mpicc not found, distribution disabled.[m
[38;5;2m[i 0109 10:52:55.566437 76 cuda_flags.cc:39] CUDA enabled.[m
Namespace(batch_size=32, cpu_count=1, data_dir='./data', decode_strategy='random', learning_rate=0.0001, maxlen=35, model_config='config.json', name='gpt-ftune32', num_epochs=30, prefix=0, pretrain_dir='pretrain/gpt2-paras.jt', temperature=1, test=None, tokenizer_dir='./tokenizer', top_k=40, top_p=1.0, train_dir='./train_test')
Tokenizer PAD ID: 50256
Loading Data ...
173
[50257,768,]
[1024,768,]
[768,]
[768,]
[1,1,1024,1024,]
[1,]
[768,2304,]
[2304,]
[768,768,]
[768,]
[768,]
[768,]
[768,3072,]
[3072,]
[3072,768,]
[768,]
[768,]
[768,]
[1,1,1024,1024,]
[1,]
[768,2304,]
[2304,]
[768,768,]
[768,]
[768,]
[768,]
[768,3072,]
[3072,]
[3072,768,]
[768,]
[768,]
[768,]
[1,1,1024,1024,]
[1,]
[768,2304,]
[2304,]
[768,768,]
[768,]
[768,]
[768,]
[768,3072,]
[3072,]
[3072,768,]
[768,]
[768,]
[768,]
[1,1,1024,1024,]
[1,]
[768,2304,]
[2304,]
[768,768,]
[768,]
[768,]
[768,]
[768,3072,]
[3072,]
[3072,768,]
[768,]
[768,]
[768,]
[1,1,1024,1024,]
[1,]
[768,2304,]
[2304,]
[768,768,]
[768,]
[768,]
[768,]
[768,3072,]
[3072,]
[3072,768,]
[768,]
[768,]
[768,]
[1,1,1024,1024,]
[1,]
[768,2304,]
[2304,]
[768,768,]
[768,]
[768,]
[768,]
[768,3072,]
[3072,]
[3072,768,]
[768,]
[768,]
[768,]
[1,1,1024,1024,]
[1,]
[768,2304,]
[2304,]
[768,768,]
[768,]
[768,]
[768,]
[768,3072,]
[3072,]
[3072,768,]
[768,]
[768,]
[768,]
[1,1,1024,1024,]
[1,]
[768,2304,]
[2304,]
[768,768,]
[768,]
[768,]
[768,]
[768,3072,]
[3072,]
[3072,768,]
[768,]
[768,]
[768,]
[1,1,1024,1024,]
[1,]
[768,2304,]
[2304,]
[768,768,]
[768,]
[768,]
[768,]
[768,3072,]
[3072,]
[3072,768,]
[768,]
[768,]
[768,]
[1,1,1024,1024,]
[1,]
[768,2304,]
[2304,]
[768,768,]
[768,]
[768,]
[768,]
[768,3072,]
[3072,]
[3072,768,]
[768,]
[768,]
[768,]
[1,1,1024,1024,]
[1,]
[768,2304,]
[2304,]
[768,768,]
[768,]
[768,]
[768,]
[768,3072,]
[3072,]
[3072,768,]
[768,]
[768,]
[768,]
[1,1,1024,1024,]
[1,]
[768,2304,]
[2304,]
[768,768,]
[768,]
[768,]
[768,]
[768,3072,]
[3072,]
[3072,768,]
[768,]
[768,]
[768,]
[50257,768,]
jittor_model info:
TfmrLMHeadModel(
    transformer: TfmrModel(
        wte: Embedding(50257, 768)
        wpe: Embedding(1024, 768)
        drop: Dropout(0.1, is_train=False)
        h: Sequential(
            0: TfmrBlock(
                ln_1: LayerNorm((768,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(2304, None, None)
                    c_proj: TransposeLinear(768, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((768,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(3072, None, None)
                    c_proj: TransposeLinear(768, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            1: TfmrBlock(
                ln_1: LayerNorm((768,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(2304, None, None)
                    c_proj: TransposeLinear(768, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((768,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(3072, None, None)
                    c_proj: TransposeLinear(768, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            2: TfmrBlock(
                ln_1: LayerNorm((768,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(2304, None, None)
                    c_proj: TransposeLinear(768, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((768,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(3072, None, None)
                    c_proj: TransposeLinear(768, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            3: TfmrBlock(
                ln_1: LayerNorm((768,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(2304, None, None)
                    c_proj: TransposeLinear(768, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((768,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(3072, None, None)
                    c_proj: TransposeLinear(768, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            4: TfmrBlock(
                ln_1: LayerNorm((768,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(2304, None, None)
                    c_proj: TransposeLinear(768, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((768,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(3072, None, None)
                    c_proj: TransposeLinear(768, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            5: TfmrBlock(
                ln_1: LayerNorm((768,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(2304, None, None)
                    c_proj: TransposeLinear(768, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((768,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(3072, None, None)
                    c_proj: TransposeLinear(768, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            6: TfmrBlock(
                ln_1: LayerNorm((768,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(2304, None, None)
                    c_proj: TransposeLinear(768, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((768,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(3072, None, None)
                    c_proj: TransposeLinear(768, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            7: TfmrBlock(
                ln_1: LayerNorm((768,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(2304, None, None)
                    c_proj: TransposeLinear(768, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((768,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(3072, None, None)
                    c_proj: TransposeLinear(768, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            8: TfmrBlock(
                ln_1: LayerNorm((768,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(2304, None, None)
                    c_proj: TransposeLinear(768, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((768,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(3072, None, None)
                    c_proj: TransposeLinear(768, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            9: TfmrBlock(
                ln_1: LayerNorm((768,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(2304, None, None)
                    c_proj: TransposeLinear(768, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((768,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(3072, None, None)
                    c_proj: TransposeLinear(768, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            10: TfmrBlock(
                ln_1: LayerNorm((768,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(2304, None, None)
                    c_proj: TransposeLinear(768, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((768,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(3072, None, None)
                    c_proj: TransposeLinear(768, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
            11: TfmrBlock(
                ln_1: LayerNorm((768,), 1e-05, elementwise_affine=True)
                attn: TfmrAttention(
                    c_attn: TransposeLinear(2304, None, None)
                    c_proj: TransposeLinear(768, None, None)
                    attn_dropout: Dropout(0.1, is_train=False)
                    resid_dropout: Dropout(0.1, is_train=False)
                )
                ln_2: LayerNorm((768,), 1e-05, elementwise_affine=True)
                mlp: TfmrMLP(
                    c_fc: TransposeLinear(3072, None, None)
                    c_proj: TransposeLinear(768, None, None)
                    dropout: Dropout(0.1, is_train=False)
                )
            )
        )
        ln_f: LayerNorm((768,), 1e-05, elementwise_affine=True)
    )
    lm_head: Linear(768, 50257, None, None)
)[38;5;3m[w 0109 10:52:57.743136 76 grad.cc:77] grads[4] 'transformer.h.0.attn.masked_bias' doesn't have gradient. It will be set to zero: Var(3101:1:1:1:i0:o0:s1:n0,float32,transformer.h.0.attn.masked_bias,7ef9d1400000)[1,][m
[38;5;3m[w 0109 10:52:57.743156 76 grad.cc:77] grads[17] 'transformer.h.1.attn.masked_bias' doesn't have gradient. It will be set to zero: Var(3129:1:1:1:i0:o0:s1:n0,float32,transformer.h.1.attn.masked_bias,7ef9d1400200)[1,][m
[38;5;3m[w 0109 10:52:57.743161 76 grad.cc:77] grads[30] 'transformer.h.2.attn.masked_bias' doesn't have gradient. It will be set to zero: Var(3157:1:1:1:i0:o0:s1:n0,float32,transformer.h.2.attn.masked_bias,7ef9d1400400)[1,][m
[38;5;3m[w 0109 10:52:57.743166 76 grad.cc:77] grads[43] 'transformer.h.3.attn.masked_bias' doesn't have gradient. It will be set to zero: Var(3185:1:1:1:i0:o0:s1:n0,float32,transformer.h.3.attn.masked_bias,7ef9d1400600)[1,][m
[38;5;3m[w 0109 10:52:57.743170 76 grad.cc:77] grads[56] 'transformer.h.4.attn.masked_bias' doesn't have gradient. It will be set to zero: Var(3213:1:1:1:i0:o0:s1:n0,float32,transformer.h.4.attn.masked_bias,7ef9d1400800)[1,][m
[38;5;3m[w 0109 10:52:57.743175 76 grad.cc:77] grads[69] 'transformer.h.5.attn.masked_bias' doesn't have gradient. It will be set to zero: Var(3241:1:1:1:i0:o0:s1:n0,float32,transformer.h.5.attn.masked_bias,7ef9d1400a00)[1,][m
[38;5;3m[w 0109 10:52:57.743179 76 grad.cc:77] grads[82] 'transformer.h.6.attn.masked_bias' doesn't have gradient. It will be set to zero: Var(3269:1:1:1:i0:o0:s1:n0,float32,transformer.h.6.attn.masked_bias,7ef9d1400c00)[1,][m
[38;5;3m[w 0109 10:52:57.743184 76 grad.cc:77] grads[95] 'transformer.h.7.attn.masked_bias' doesn't have gradient. It will be set to zero: Var(3297:1:1:1:i0:o0:s1:n0,float32,transformer.h.7.attn.masked_bias,7ef9d1400e00)[1,][m
[38;5;3m[w 0109 10:52:57.743188 76 grad.cc:77] grads[108] 'transformer.h.8.attn.masked_bias' doesn't have gradient. It will be set to zero: Var(3325:1:1:1:i0:o0:s1:n0,float32,transformer.h.8.attn.masked_bias,7ef9d1401000)[1,][m
[38;5;3m[w 0109 10:52:57.743192 76 grad.cc:77] grads[121] 'transformer.h.9.attn.masked_bias' doesn't have gradient. It will be set to zero: Var(3353:1:1:1:i0:o0:s1:n0,float32,transformer.h.9.attn.masked_bias,7ef9d1401200)[1,][m
[38;5;3m[w 0109 10:52:57.743197 76 grad.cc:77] grads[134] 'transformer.h.10.attn.masked_bias' doesn't have gradient. It will be set to zero: Var(3381:1:1:1:i0:o0:s1:n0,float32,transformer.h.10.attn.masked_bias,7ef9d1401400)[1,][m
[38;5;3m[w 0109 10:52:57.743202 76 grad.cc:77] grads[147] 'transformer.h.11.attn.masked_bias' doesn't have gradient. It will be set to zero: Var(3409:1:1:1:i0:o0:s1:n0,float32,transformer.h.11.attn.masked_bias,7ef9d1401600)[1,][m

Compiling Operators(23/39) used: 3.32s eta: 2.31s Compiling Operators(37/39) used: 4.32s eta: 0.233s Compiling Operators(39/39) used: 6.32s eta:    0s 

Compiling Operators(2/2) used: 2.31s eta:    0s 

Epoch 1 Batch 10, train loss 12.422607
Epoch 1 Batch 20, train loss 11.493230
Epoch 1 Batch 30, train loss 10.985891
Epoch 1 Batch 40, train loss 10.469954
Epoch 1 Batch 50, train loss 9.706527
Epoch 1 Batch 60, train loss 9.025668
Epoch 1 Batch 70, train loss 8.496088
Epoch 1 Batch 80, train loss 8.067132
Epoch 1 Batch 90, train loss 7.778878
Epoch 1 Batch 100, train loss 7.523975
Epoch 1 Batch 110, train loss 6.785032
Epoch 1 Batch 120, train loss 6.214016
Epoch 1 Batch 130, train loss 5.684703
Epoch 1 Batch 140, train loss 5.233634
Epoch 1 Batch 150, train loss 5.012127
Epoch 1 Batch 160, train loss 4.867244
Epoch 1 Batch 170, train loss 4.738512
Epoch 1 Batch 180, train loss 4.613348
Epoch 1 Batch 190, train loss 4.453857
Epoch 1 Batch 200, train loss 4.304772
Epoch 1 Batch 210, train loss 4.167164
Epoch 1 Batch 220, train loss 4.044349
Epoch 1 Batch 230, train loss 3.919572
Epoch 1 Batch 240, train loss 3.818592
Epoch 1 Batch 250, train loss 3.723333
Epoch 1 Batch 260, train loss 3.648540
Epoch 1 Batch 270, train loss 3.561485
Epoch 1 Batch 280, train loss 3.512681
Epoch 1 Batch 290, train loss 3.451860
Epoch 1 Batch 300, train loss 3.419391
Epoch 1 Batch 310, train loss 3.395806
Epoch 1 Batch 320, train loss 3.361929
Epoch 1 Batch 330, train loss 3.330650
Epoch 1 Batch 340, train loss 3.304610
Epoch 1 Batch 350, train loss 3.238517
Epoch 1 Batch 360, train loss 3.183652
Epoch 1 Batch 370, train loss 3.165234
Epoch 1 Batch 380, train loss 3.111330
Epoch 1 Batch 390, train loss 3.054430
Epoch 1 Batch 400, train loss 2.966445
Epoch 1 Batch 410, train loss 2.870277
Epoch 1 Batch 420, train loss 2.790358
Epoch 1 Batch 430, train loss 2.714996
Epoch 1 Batch 440, train loss 2.640374
Epoch 1 Batch 450, train loss 2.588441
Epoch 1 Batch 460, train loss 2.521268
Epoch 1 of 30 took 74.75771880149841s
  training loss:                 4.233082300056019
  validation loss:               3.2859893360435963
  validation perplexity:         26.735421551256398
  best epoch:                    1
  best validation perplexity:    26.735421551256398
Epoch 2 Batch 10, train loss 3.030329
Epoch 2 Batch 20, train loss 3.031967
Epoch 2 Batch 30, train loss 3.105094
Epoch 2 Batch 40, train loss 3.138032
Epoch 2 Batch 50, train loss 3.135161
Epoch 2 Batch 60, train loss 3.148269
Epoch 2 Batch 70, train loss 3.146427
Epoch 2 Batch 80, train loss 3.150783
Epoch 2 Batch 90, train loss 3.125658
Epoch 2 Batch 100, train loss 3.097288
Epoch 2 Batch 110, train loss 3.084156
Epoch 2 Batch 120, train loss 3.047138
Epoch 2 Batch 130, train loss 2.997583
Epoch 2 Batch 140, train loss 2.932498
Epoch 2 Batch 150, train loss 2.901480
Epoch 2 Batch 160, train loss 2.849671
Epoch 2 Batch 170, train loss 2.808595
Epoch 2 Batch 180, train loss 2.759594
Epoch 2 Batch 190, train loss 2.738617
Epoch 2 Batch 200, train loss 2.723787
Epoch 2 Batch 210, train loss 2.703179
Epoch 2 Batch 220, train loss 2.706027
Epoch 2 Batch 230, train loss 2.687744
Epoch 2 Batch 240, train loss 2.691107
Epoch 2 Batch 250, train loss 2.681849
Epoch 2 Batch 260, train loss 2.676541
Epoch 2 Batch 270, train loss 2.654563
Epoch 2 Batch 280, train loss 2.648535
Epoch 2 Batch 290, train loss 2.647268
Epoch 2 Batch 300, train loss 2.633242
Epoch 2 Batch 310, train loss 2.593175
Epoch 2 Batch 320, train loss 2.557918
Epoch 2 Batch 330, train loss 2.525520
Epoch 2 Batch 340, train loss 2.496616
Epoch 2 Batch 350, train loss 2.435126
Epoch 2 Batch 360, train loss 2.393237
Epoch 2 Batch 370, train loss 2.375794
Epoch 2 Batch 380, train loss 2.334428
Epoch 2 Batch 390, train loss 2.280337
Epoch 2 Batch 400, train loss 2.230016
Epoch 2 Batch 410, train loss 2.203684
Epoch 2 Batch 420, train loss 2.179346
Epoch 2 Batch 430, train loss 2.153379
Epoch 2 Batch 440, train loss 2.125146
Epoch 2 Batch 450, train loss 2.115186
Epoch 2 Batch 460, train loss 2.087710
Epoch 2 of 30 took 22.4807186126709s
  training loss:                 2.579167527176424
  validation loss:               3.2859893238306044
  validation perplexity:         26.73542122473691
  best epoch:                    2
  best validation perplexity:    26.73542122473691
Epoch 3 Batch 10, train loss 3.030329
Epoch 3 Batch 20, train loss 3.031967
Epoch 3 Batch 30, train loss 3.105094
Epoch 3 Batch 40, train loss 3.138032
Epoch 3 Batch 50, train loss 3.135161
Epoch 3 Batch 60, train loss 3.148269
Epoch 3 Batch 70, train loss 3.146427
Epoch 3 Batch 80, train loss 3.150783
Epoch 3 Batch 90, train loss 3.125658
Epoch 3 Batch 100, train loss 3.097288
Epoch 3 Batch 110, train loss 3.084156
Epoch 3 Batch 120, train loss 3.047138
Epoch 3 Batch 130, train loss 2.997583
Epoch 3 Batch 140, train loss 2.932498
Epoch 3 Batch 150, train loss 2.901480
Epoch 3 Batch 160, train loss 2.849671
Epoch 3 Batch 170, train loss 2.808595
Epoch 3 Batch 180, train loss 2.759594
Epoch 3 Batch 190, train loss 2.738617
Epoch 3 Batch 200, train loss 2.723787
Epoch 3 Batch 210, train loss 2.703179
Epoch 3 Batch 220, train loss 2.706027
Epoch 3 Batch 230, train loss 2.687744
Epoch 3 Batch 240, train loss 2.691107
Epoch 3 Batch 250, train loss 2.681849
Epoch 3 Batch 260, train loss 2.676541
Epoch 3 Batch 270, train loss 2.654563
Epoch 3 Batch 280, train loss 2.648535
Epoch 3 Batch 290, train loss 2.647268
Epoch 3 Batch 300, train loss 2.633243
Epoch 3 Batch 310, train loss 2.593175
Epoch 3 Batch 320, train loss 2.557918
Epoch 3 Batch 330, train loss 2.525520
Epoch 3 Batch 340, train loss 2.496616
Epoch 3 Batch 350, train loss 2.435126
Epoch 3 Batch 360, train loss 2.393237
Epoch 3 Batch 370, train loss 2.375794
Epoch 3 Batch 380, train loss 2.334428
Epoch 3 Batch 390, train loss 2.280337
Epoch 3 Batch 400, train loss 2.230016
Epoch 3 Batch 410, train loss 2.203684
Epoch 3 Batch 420, train loss 2.179346
Epoch 3 Batch 430, train loss 2.153379
Epoch 3 Batch 440, train loss 2.125146
Epoch 3 Batch 450, train loss 2.115186
Epoch 3 Batch 460, train loss 2.087710
Validation loss: 26.735, becomes larger. Stop training.
